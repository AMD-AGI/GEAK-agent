# LLM model
api_key: ""
model_id: "dvue-aoai-001-gpt-4.1"
temperature: 1.0

# TritonBench
statis_path: "./dataloaders/TB_eval/data/TritonBench_G_comp_alpac_v1_hackathon.json"
py_folder: "./dataloaders/TB_eval/data/TritonBench_G_v1_hackathon"
instruction_path: "./dataloaders/TB_eval/data/TritonBench_G_comp_alpac_v1_hackathon.json"
corpus_path: "./dataloaders/TB_eval/data/train_crawl.json"
golden_metrics: "/group/ossdphi_algo_scratch_03/jianghui/projects/kernel_agent/TB-eval-release/tb_eval/data/TritonBench/performance_metrics/perf_G/golden_metrics"
perf_ref_folder: null
perf_G_path: "./dataloaders/TB_eval/data/performance_metrics"
py_interpreter: "python"

# configs for resuming optimization process
result_path: null
mem_file: null
start_iter: 0

# target_kernels: ["flash_decode2_phi.py", 'cosine_compute.py', "int4_matmul.py", "matmul_leakyrelu.py", "triton_matmul.py", "l2_norm_bwd.py", "swiglu_fwd.py", "embedding_triton_kernel.py", "rotary_transform.py", "matrix_vector_multip.py"]
# you can specify which kernels you want to generate by setting target_kernels. null means all 10 kernels for hackathon.
target_kernels: null

# the path where results will be stored
output_path: "../outputs/reflexion_oneshot_tritonbench.json"
max_iteration: 10
# set multi_thread to false if you want to debug the process
multi_thread: true

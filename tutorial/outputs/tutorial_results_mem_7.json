{"grouped_gemm_triton_kernel.py": {"call_err_msg": "None", "exe_err_msg": "Generated output is None", "reflection": null, "oneshot": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_moe_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    a_scale_ptr,\n    b_scale_ptr,\n    topk_weights_ptr,\n    sorted_token_ids_ptr,\n    expert_ids_ptr,\n    num_tokens_post_padded_ptr,\n    N,\n    K,\n    EM,\n    num_valid_tokens,\n    stride_am,\n    stride_ak,\n    stride_be,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    MUL_ROUTED_WEIGHT: tl.constexpr,\n    top_k: tl.constexpr,\n    compute_type: tl.constexpr,\n    use_fp8: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +\n                      offs_k[None, :] * stride_ak)\n\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n                                                offs_bn[None, :] * stride_bn)\n\n    if use_fp8:\n        a_scale = tl.load(a_scale_ptr)\n        b_scale = tl.load(b_scale_ptr + off_experts)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs,\n                    mask=token_mask[:, None] &\n                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),\n                    other=0.0)\n        b = tl.load(b_ptrs,\n                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n                    other=0.0)\n        if use_fp8:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        else:\n            accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token,\n                             mask=token_mask,\n                             other=0)\n        accumulator = accumulator * moe_weight[:, None]\n\n    if use_fp8:\n        accumulator = (accumulator * a_scale * b_scale).to(compute_type)\n    else:\n        accumulator = accumulator.to(compute_type)\n\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n\ndef invoke_fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor,\n                            A_scale: Optional[torch.Tensor],\n                            B_scale: Optional[torch.Tensor],\n                            topk_weights: torch.Tensor, topk_ids: torch.Tensor,\n                            sorted_token_ids: torch.Tensor,\n                            expert_ids: torch.Tensor,\n                            num_tokens_post_padded: torch.Tensor,\n                            mul_routed_weight: bool, top_k: int,\n                            config: Dict[str, Any], compute_type: tl.dtype,\n                            use_fp8: bool) -> None:\n    grid = lambda META: (triton.cdiv(sorted_token_ids.shape[0], META[\n        'BLOCK_SIZE_M']) * triton.cdiv(B.shape[1], META['BLOCK_SIZE_N']), )\n\n    fused_moe_kernel[grid](\n        A,\n        B,\n        C,\n        A_scale,\n        B_scale,\n        topk_weights,\n        sorted_token_ids,\n        expert_ids,\n        num_tokens_post_padded,\n        B.shape[1],\n        B.shape[2],\n        sorted_token_ids.shape[0],\n        topk_ids.numel(),\n        A.stride(0),\n        A.stride(1),\n        B.stride(0),\n        B.stride(2),\n        B.stride(1),\n        C.stride(1),\n        C.stride(2),\n        MUL_ROUTED_WEIGHT=mul_routed_weight,\n        top_k=top_k,\n        compute_type=compute_type,\n        use_fp8=use_fp8,\n        **config,\n    )\n", "perf_candidates": [], "perf_strategy": null, "call_candidate": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    # 1. Map PID to Group and Tile\n    pid = tl.program_id(axis=0)\n    group_id = tl.load(tile_to_group_ptr + pid)\n    tile_idx = tl.load(tile_offset_ptr + pid)\n\n    # 2. Load Problem Dimensions for this Group\n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n\n    # 3. Calculate Tile Coordinates (Row-Major Tiling)\n    num_tiles_n = (N + BLOCK_N - 1) // BLOCK_N\n    block_m_idx = tile_idx // num_tiles_n\n    block_n_idx = tile_idx % num_tiles_n\n\n    # 4. Calculate Offsets\n    offs_m = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # 5. Determine Memory Locations\n    # A: Concatenated [Sum(M), K]. Use M_cumsum to find start row.\n    m_start_row = tl.load(M_cumsum_ptr + group_id)\n    \n    # Pointer for A\n    # A is accessed as [row, k]. stride_ak is stride of M dimension.\n    a_base = A_ptr + (m_start_row * stride_ak)\n    a_ptrs = a_base + (offs_m[:, None] * stride_ak + offs_k[None, :])\n\n    # Pointer for B\n    # B is specific to the group, address stored in B_ptr_array\n    b_ptr_int = tl.load(B_ptr_array + group_id)\n    b_base = b_ptr_int.to(tl.pointer_type(tl.float8e4b8))\n    b_ptrs = b_base + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n    # Scales: Assumed layout [scale_a_g0, scale_b_g0, scale_a_g1, ...]\n    scale_a = tl.load(scales_ptr + 2 * group_id)\n    scale_b = tl.load(scales_ptr + 2 * group_id + 1)\n\n    # 6. Main Loop\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, K, BLOCK_K):\n        # Masks\n        mask_m = offs_m < M\n        mask_n = offs_n < N\n        mask_k = (offs_k + k) < K\n\n        # Load Tiles\n        # Note: A assumes K is contiguous (stride 1) or handled by stride_ak if not.\n        # The construction of a_ptrs assumes inner dim stride is 1 for offs_k scaling.\n        # If stride_ak is passed, it manages row steps.\n        \n        a_tile = tl.load(a_ptrs, mask=(mask_m[:, None] & mask_k[None, :]), other=0.0)\n        b_tile = tl.load(b_ptrs, mask=(mask_k[:, None] & mask_n[None, :]), other=0.0)\n\n        # Explicit Cast as required by constraints\n        a_fp8 = a_tile.to(tl.float8e4b8)\n        b_fp8 = b_tile.to(tl.float8e4b8)\n\n        # Dot Product\n        accumulator = tl.dot(a_fp8, b_fp8, accumulator)\n\n        # Advance Pointers\n        a_ptrs += BLOCK_K # Advance K dim (stride 1)\n        b_ptrs += BLOCK_K * stride_bk # Advance K dim\n\n    # 7. Epilogue\n    # Dequantize\n    c_result = accumulator * scale_a * scale_b\n    c_result = c_result.to(tl.bfloat16)\n\n    # Store C\n    # C is concatenated [Sum(M), N]. Shared N across groups (usually) or Ragged.\n    # Using stride_ck to handle row steps.\n    c_base = C_ptr + (m_start_row * stride_ck)\n    c_ptrs = c_base + (offs_m[:, None] * stride_ck + offs_n[None, :])\n    \n    mask_c = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c_result, mask=mask_c)\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: list[torch.Tensor], device='cuda'):\n        self.device = device\n        # Keep B tensors alive and get their pointers\n        self.B_list = [b.to(device) for b in B_list]\n        self.num_groups = len(B_list)\n        \n        # Extract N and K from B_list (Assuming B is [K, N] or [N, K] depending on usage)\n        # We assume standard Linear layer weights: [Out_Features, In_Features] = [N, K].\n        # But usually GEMM expects B as [K, N]. Check strides in call.\n        # Let's assume input B_list are properly shaped for the matmul.\n        self.Ks = [b.shape[0] for b in self.B_list]\n        self.Ns = [b.shape[1] for b in self.B_list]\n        \n        # Verify consistency (K should be same for concat A)\n        self.K = self.Ks[0]\n        \n        # Pre-allocate B_ptr_array\n        b_ptrs = [b.data_ptr() for b in self.B_list]\n        self.B_ptr_array = torch.tensor(b_ptrs, dtype=torch.int64, device=self.device)\n        \n        # Cache for schedules\n        self.cache = {}\n\n    def _compute_schedule(self, M_splits: list[int], BLOCK_M=128, BLOCK_N=128):\n        tile_to_group = []\n        tile_offset = []\n        \n        total_tiles = 0\n        for group_id, m in enumerate(M_splits):\n            n = self.Ns[group_id]\n            \n            # Tiles in M and N\n            num_m_tiles = (m + BLOCK_M - 1) // BLOCK_M\n            num_n_tiles = (n + BLOCK_N - 1) // BLOCK_N\n            num_group_tiles = num_m_tiles * num_n_tiles\n            \n            # Add to schedule\n            tile_to_group.extend([group_id] * num_group_tiles)\n            tile_offset.extend(range(num_group_tiles))\n            \n            total_tiles += num_group_tiles\n            \n        return (\n            torch.tensor(tile_to_group, dtype=torch.int32, device=self.device),\n            torch.tensor(tile_offset, dtype=torch.int32, device=self.device),\n            total_tiles\n        )\n\n    def _get_schedule(self, M_splits: list[int]):\n        # Simple caching based on tuple of splits\n        key = tuple(M_splits)\n        if key not in self.cache:\n            # Default block sizes for FP8\n            self.cache[key] = self._compute_schedule(M_splits, BLOCK_M=128, BLOCK_N=128)\n        return self.cache[key]\n\n    def __call__(self, A_concat: torch.Tensor, M_splits: list[int]):\n        # 1. Prepare Metadata\n        tile_to_group, tile_offset, total_tiles = self._get_schedule(M_splits)\n        \n        # 2. Prepare Auxiliary Tensors\n        M_array = torch.tensor(M_splits, dtype=torch.int32, device=self.device)\n        N_array = torch.tensor(self.Ns, dtype=torch.int32, device=self.device)\n        \n        # Cumulative Sum for A offsets\n        M_cumsum = torch.cumsum(torch.tensor([0] + M_splits, dtype=torch.int32, device=self.device), dim=0)\n        \n        # Scales: Dummy scales for this example (1.0). In real usage, pass as arg.\n        # Requirements say 'scales_ptr' is passed. Creating dummy per-tensor scales.\n        # Shape: [2 * num_groups] (scale_a, scale_b for each group)\n        scales = torch.ones(2 * self.num_groups, dtype=torch.float32, device=self.device)\n        \n        # Output Alloc\n        total_M = sum(M_splits)\n        max_N = max(self.Ns)\n        # Assuming all N are same for concatenated output, or C is effectively 1D/ragged buffer.\n        # Standard MoE: C is same shape as A_concat but with dim N.\n        C_concat = torch.empty((total_M, max_N), dtype=torch.bfloat16, device=self.device)\n        \n        # 3. Kernel Config\n        BLOCK_M = 128\n        BLOCK_N = 128\n        BLOCK_K = 128\n        num_stages = 3\n        \n        grid = (total_tiles,)\n        \n        # Strides\n        stride_ak = A_concat.stride(0)\n        # Assuming B matrices are [K, N] row-major in memory for this example\n        stride_bk = self.B_list[0].stride(0)\n        stride_bn = self.B_list[0].stride(1)\n        stride_ck = C_concat.stride(0)\n\n        # 4. Launch\n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_ptr=A_concat,\n            C_ptr=C_concat,\n            B_ptr_array=self.B_ptr_array,\n            M_cumsum_ptr=M_cumsum,\n            M_array=M_array,\n            N_array=N_array,\n            K=self.K,\n            scales_ptr=scales,\n            tile_to_group_ptr=tile_to_group,\n            tile_offset_ptr=tile_offset,\n            stride_ak=stride_ak,\n            stride_bk=stride_bk,\n            stride_bn=stride_bn,\n            stride_ck=stride_ck,\n            total_tiles=total_tiles,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n            BLOCK_K=BLOCK_K,\n            GROUP_SIZE_M=8, # unused in flattened schedule but kept for signature\n            NUM_GROUPS=self.num_groups,\n            num_stages=num_stages,\n        )\n        \n        return C_concat\n\ndef test_grouped_gemm():\n    torch.manual_seed(0)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if device.type == 'cpu':\n        print(\"Skipping test on CPU\")\n        return\n\n    # Setup\n    num_groups = 4\n    K = 256\n    N = 256\n    M_splits = [64, 128, 32, 256]\n    total_M = sum(M_splits)\n    \n    # Inputs\n    # Using float16 for inputs to simulate, kernel will cast to FP8\n    # In real usage, inputs might already be FP8.\n    A = torch.randn((total_M, K), dtype=torch.float16, device=device)\n    B_list = [torch.randn((K, N), dtype=torch.float16, device=device) for _ in range(num_groups)]\n    \n    gemm_op = OptimizedGroupedGEMM(B_list, device=device)\n    C_out = gemm_op(A, M_splits)\n    \n    print(f\"Kernel Output Shape: {C_out.shape}\")\n    print(\"Kernel executed successfully.\")\n\nif __name__ == \"__main__\":\n    test_grouped_gemm()\n", "exe_candidate": null, "temp_strategy": "Implement a flattened Triton kernel where each program instance (PID) retrieves its workload (Group ID and Tile Offset) from pre-calculated schedule tensors ('tile_to_group_ptr', 'tile_offset_ptr'). The kernel uses indirect memory access via 'B_ptr_array' to handle non-contiguous weight matrices for each group. Input A is treated as a concatenated ragged tensor, accessed using 'M_cumsum_ptr' offsets. Standard pointer arithmetic with 'tl.load' and masking is used instead of block pointers. Inputs are explicitly cast to 'tl.float8e4b8' immediately before 'tl.dot' to ensure type correctness on AMD ROCm. The host class 'OptimizedGroupedGEMM' manages the scheduling logic, pre-computing the grid layout and managing memory pointers.", "perf_debug_num": 0, "pass_call": true, "pass_exe": true, "pass_perf": false}}
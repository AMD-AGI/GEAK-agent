{"instruction": "Develop an optimized Triton kernel for Fused Grouped FP8 GEMM aimed at Mixture-of-Experts (MoE) workloads. The kernel must support variable-sized groups (Ragged Batching) where multiple GEMM operations share weights (B) but have different input M dimensions (A). Key requirements include: 1) FP8 support (E4M3) with per-tensor scaling factors for quantization/dequantization, 2) Grouped GEMM scheduling where 'tile_to_group_ptr' maps every tile to its group ID for load balancing, 3) Use a pointer-array ('B_ptr_array') for non-contiguous weight matrices, 4) Wrap host logic in a class 'OptimizedGroupedGEMM', 5) Include a 'test_grouped_gemm' function and 'if __name__ == \"__main__\":' block. CRITICAL CONSTRAINTS: 6) Use standard pointer arithmetic (tl.load with offsets). Do NOT use 'tl.make_block_ptr'. 7) DO NOT define or call any helper functions (like 'get_fp8_dtype') inside the JIT kernel. 8) DO NOT import 'transformer_engine'. Use only 'torch' and 'triton'. 9) For the FP8 data type, you MUST use 'tl.float8e4b8' directly. Do not use 'tl.float8e4m3fn'. 10) The kernel's output should be bfloat16. 11) When performing FP8 matrix multiplication using tl.dot, explicitly cast both input operands to the target FP8 data type (e.g., tl.float8e4b8) immediately before the dot operation to prevent data type errors.", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n    ],\n    key=['total_tiles', 'K'],\n)\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    group_id = tl.load(tile_to_group_ptr + pid)\n    local_tile_id = tl.load(tile_offset_ptr + pid)\n    \n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n    m_offset = tl.load(M_cumsum_ptr + group_id)\n    \n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id_tile = local_tile_id // num_pid_in_group\n    first_pid_m = group_id_tile * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (local_tile_id % group_size_m)\n    pid_n = (local_tile_id % num_pid_in_group) // group_size_m\n    \n    scale_base = group_id * 4\n    qscale_a = tl.load(scales_ptr + scale_base + 0)\n    qscale_b = tl.load(scales_ptr + scale_base + 1)\n    dscale_a = tl.load(scales_ptr + scale_base + 2)\n    dscale_b = tl.load(scales_ptr + scale_base + 3)\n    \n    b_ptr = tl.load(B_ptr_array + group_id).to(tl.pointer_type(tl.bfloat16))\n    \n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = A_ptr + (m_offset + offs_am[:, None]) * stride_ak + offs_k[None, :]\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        \n        # --- M/N/K Masking ---\n        # A mask: check M bounds (rows) and K bounds (cols)\n        k_mask_a = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        \n        # B mask: check N bounds (cols) and K bounds (rows)\n        k_mask_b = (offs_bn[None, :] < N) & (offs_k[:, None] < k_remaining)\n        \n        a_bf16 = tl.load(a_ptrs, mask=k_mask_a, other=0.0)\n        b_bf16 = tl.load(b_ptrs, mask=k_mask_b, other=0.0)\n        \n        a_fp8 = (a_bf16 * qscale_a).to(tl.float8e4b8)\n        b_fp8 = (b_bf16 * qscale_b).to(tl.float8e4b8)\n        \n        accumulator = tl.dot(a_fp8, b_fp8, accumulator)\n        \n        a_ptrs += BLOCK_K\n        b_ptrs += BLOCK_K * stride_bk\n    \n    total_dscale = dscale_a * dscale_b\n    c = accumulator * total_dscale\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = C_ptr + (m_offset + offs_cm[:, None]) * stride_ck + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    \n    tl.store(c_ptrs, c.to(tl.bfloat16), mask=c_mask)\n\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        self.num_groups = len(B_list)\n        self.K = B_list[0].shape[0]\n        self.N = B_list[0].shape[1]\n        self.FP8_MAX = 240.0\n        \n        self.B_list = [b.contiguous() for b in B_list]\n        self.B_ptr_array = torch.tensor(\n            [b.data_ptr() for b in self.B_list],\n            device=device, dtype=torch.int64\n        )\n        \n        self.B_qscales = torch.empty(self.num_groups, device=device, dtype=torch.float32)\n        self.B_dscales = torch.empty(self.num_groups, device=device, dtype=torch.float32)\n        for i, B in enumerate(self.B_list):\n            max_b = B.abs().max().float().item()\n            max_b = max(max_b, 1e-6)\n            self.B_qscales[i] = self.FP8_MAX / max_b\n            self.B_dscales[i] = max_b / self.FP8_MAX\n        \n        self.scales_buffer = torch.empty(self.num_groups * 4, device=device, dtype=torch.float32)\n        for i in range(self.num_groups):\n            self.scales_buffer[i*4 + 1] = self.B_qscales[i]\n            self.scales_buffer[i*4 + 3] = self.B_dscales[i]\n        \n        self._cached_m_splits = None\n        self._cached_schedule = None\n        self._cached_C_out = None\n        \n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        M_cumsum = [0]\n        tile_to_group = []\n        tile_offset = []\n        M_array = []\n        N_array = []\n        \n        for group_id, M in enumerate(M_splits):\n            M_cumsum.append(M_cumsum[-1] + M)\n            M_array.append(M)\n            N_array.append(self.N)\n            \n            num_tiles_m = triton.cdiv(M, BLOCK_M)\n            num_tiles_n = triton.cdiv(self.N, BLOCK_N)\n            \n            for local_id in range(num_tiles_m * num_tiles_n):\n                tile_to_group.append(group_id)\n                tile_offset.append(local_id)\n        \n        return {\n            'M_cumsum': torch.tensor(M_cumsum[:-1], device=self.device, dtype=torch.int32),\n            'M_array': torch.tensor(M_array, device=self.device, dtype=torch.int32),\n            'N_array': torch.tensor(N_array, device=self.device, dtype=torch.int32),\n            'tile_to_group': torch.tensor(tile_to_group, device=self.device, dtype=torch.int32),\n            'tile_offset': torch.tensor(tile_offset, device=self.device, dtype=torch.int32),\n            'total_tiles': len(tile_to_group),\n            'total_M': sum(M_splits),\n        }\n    \n    def _get_schedule(self, M_splits: List[int]):\n        m_tuple = tuple(M_splits)\n        if self._cached_m_splits != m_tuple:\n            self._cached_schedule = self._compute_schedule(M_splits)\n            self._cached_m_splits = m_tuple\n            total_M = self._cached_schedule['total_M']\n            self._cached_C_out = torch.empty((total_M, self.N), device=self.device, dtype=torch.bfloat16)\n        return self._cached_schedule\n    \n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]) -> torch.Tensor:\n        schedule = self._get_schedule(M_splits)\n        \n        start = 0\n        for i, m in enumerate(M_splits):\n            max_a = A_concat[start:start+m].abs().max().float().item()\n            max_a = max(max_a, 1e-6)\n            qscale_a = self.FP8_MAX / max_a\n            self.scales_buffer[i*4 + 0] = qscale_a\n            self.scales_buffer[i*4 + 2] = 1.0 / qscale_a\n            start += m\n        \n        grid = (schedule['total_tiles'],)\n        \n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_concat, self._cached_C_out,\n            self.B_ptr_array,\n            schedule['M_cumsum'],\n            schedule['M_array'], schedule['N_array'], self.K,\n            self.scales_buffer,\n            schedule['tile_to_group'], schedule['tile_offset'],\n            A_concat.stride(0), self.B_list[0].stride(0), self.B_list[0].stride(1), \n            self._cached_C_out.stride(0),\n            schedule['total_tiles'],\n            NUM_GROUPS=self.num_groups,\n        )\n        \n        return self._cached_C_out\n", "filename": "grouped_gemm_triton_kernel.py", "predict": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List\n\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    # Program ID\n    pid = tl.program_id(axis=0)\n    \n    # 1. Schedule Retrieval\n    # Identify which group and which tile within that group this PID is responsible for\n    group_id = tl.load(tile_to_group_ptr + pid)\n    tile_idx = tl.load(tile_offset_ptr + pid)\n    \n    # 2. Group Metadata Loading\n    # Load M and N dimensions for the specific group\n    M_g = tl.load(M_array + group_id)\n    N_g = tl.load(N_array + group_id)\n    \n    # 3. Coordinate Calculation\n    # Recover pid_m and pid_n from the linearized tile_idx\n    num_n_tiles = (N_g + BLOCK_N - 1) // BLOCK_N\n    pid_m = tile_idx // num_n_tiles\n    pid_n = tile_idx % num_n_tiles\n    \n    # Calculate the starting row offset for A and C in the concatenated tensor\n    M_start = tl.load(M_cumsum_ptr + group_id)\n    \n    # 4. Pointer Arithmetic generation\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    # A Pointer Setup: A is [Total_M, K]\n    # Address = Base + (Row * StrideM) + (Col * StrideK)\n    # Assuming stride_ak is the stride of the M dimension (row stride)\n    a_ptrs = A_ptr + (M_start + offs_m[:, None]) * stride_ak + offs_k[None, :]\n    \n    # B Pointer Setup: B is stored as a list of pointers\n    # Load the address of the specific B matrix for this group\n    b_ptr_addr = tl.load(B_ptr_array + group_id)\n    # Cast the int64 address to a pointer to float8e4b8\n    b_base_ptr = b_ptr_addr.to(tl.pointer_type(tl.float8e4b8))\n    # B Address = Base + (K_index * StrideK) + (N_index * StrideN)\n    b_ptrs = b_base_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    \n    # Scales Loading\n    # Layout assumption: [scale_a_g0, scale_b_g0, scale_a_g1, scale_b_g1, ...]\n    scale_a = tl.load(scales_ptr + 2 * group_id)\n    scale_b = tl.load(scales_ptr + 2 * group_id + 1)\n    \n    # Accumulator initialization\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # 5. GEMM Loop\n    for k in range(0, K, BLOCK_K):\n        # Boundary checks\n        # A mask: Check if m is within M_g and if k is within K\n        a_mask = (offs_m[:, None] < M_g) & (offs_k[None, :] + k < K)\n        # B mask: Check if n is within N_g and if k is within K\n        b_mask = (offs_n[None, :] < N_g) & (offs_k[:, None] + k < K)\n        \n        # Load blocks\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        # CRITICAL: Explicit cast to target FP8 type before dot\n        a = a.to(tl.float8e4b8)\n        b = b.to(tl.float8e4b8)\n        \n        # Matrix Multiplication\n        accumulator = tl.dot(a, b, accumulator)\n        \n        # Advance pointers\n        a_ptrs += BLOCK_K\n        b_ptrs += BLOCK_K * stride_bk\n    \n    # 6. Epilogue\n    # Apply scales\n    c = accumulator * scale_a * scale_b\n    # Cast to output type\n    c = c.to(tl.bfloat16)\n    \n    # Store Result\n    # C Pointer Setup: C is [Total_M, Max_N] (Ragged handled by stride_ck or user layout)\n    # Address = Base + (Row * StrideM) + (Col * StrideN)\n    c_ptrs = C_ptr + (M_start + offs_m[:, None]) * stride_ck + offs_n[None, :]\n    c_mask = (offs_m[:, None] < M_g) & (offs_n[None, :] < N_g)\n    tl.store(c_ptrs, c, mask=c_mask)\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        # Store metadata for B tensors\n        self.B_ptrs = torch.tensor([b.data_ptr() for b in B_list], dtype=torch.int64, device=device)\n        self.N_array = torch.tensor([b.shape[1] for b in B_list], dtype=torch.int32, device=device)\n        self.K = B_list[0].shape[0]\n        \n        # Calculate strides from the first B tensor (assuming all have same layout)\n        # B shape is usually [K, N]\n        self.stride_bk = B_list[0].stride(0)\n        self.stride_bn = B_list[0].stride(1)\n        \n        self.cache = {}\n\n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        tile_to_group = []\n        tile_offset = []\n        \n        # Iterate over each group to calculate tiles\n        for g_id, m in enumerate(M_splits):\n            n = self.N_array[g_id].item()\n            num_m_tiles = (m + BLOCK_M - 1) // BLOCK_M\n            num_n_tiles = (n + BLOCK_N - 1) // BLOCK_N\n            \n            # Generate linear tile indices for this group\n            total_group_tiles = num_m_tiles * num_n_tiles\n            for t in range(total_group_tiles):\n                tile_to_group.append(g_id)\n                tile_offset.append(t)\n                \n        return torch.tensor(tile_to_group, dtype=torch.int32, device=self.device), \\\n               torch.tensor(tile_offset, dtype=torch.int32, device=self.device)\n\n    def _get_schedule(self, M_splits: List[int]):\n        # Simple caching mechanism based on M distribution\n        key = tuple(M_splits)\n        if key not in self.cache:\n            self.cache[key] = self._compute_schedule(M_splits)\n        return self.cache[key]\n\n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]):\n        # 1. Prepare Schedule\n        tile_to_group, tile_offset = self._get_schedule(M_splits)\n        total_tiles = tile_to_group.shape[0]\n        \n        # 2. Prepare Metadata Tensors\n        M_array = torch.tensor(M_splits, dtype=torch.int32, device=self.device)\n        # Calculate cumulative sum for M to find start offsets in A/C\n        zeros = torch.zeros(1, dtype=torch.int32, device=self.device)\n        M_cumsum = torch.cumsum(torch.cat([zeros, M_array]), dim=0)\n        M_cumsum_ptr = M_cumsum[:-1] # Pointer to the start of each group\n        \n        # 3. Output Allocation\n        # Allocate C with shape [Total_M, Max_N]\n        max_N = self.N_array.max().item()\n        total_M = sum(M_splits)\n        C = torch.empty((total_M, max_N), dtype=torch.bfloat16, device=self.device)\n        \n        # 4. Dummy Scales (Requirement: per-tensor scaling, but not provided in __call__)\n        # Creating a tensor of 1.0s. Format: [Scale_A_G0, Scale_B_G0, ...]\n        num_groups = len(M_splits)\n        scales = torch.ones(num_groups * 2, dtype=torch.float32, device=self.device)\n        \n        # 5. Launch Kernel\n        grid = (total_tiles,)\n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_ptr=A_concat,\n            C_ptr=C,\n            B_ptr_array=self.B_ptrs,\n            M_cumsum_ptr=M_cumsum_ptr,\n            M_array=M_array,\n            N_array=self.N_array,\n            K=self.K,\n            scales_ptr=scales,\n            tile_to_group_ptr=tile_to_group,\n            tile_offset_ptr=tile_offset,\n            stride_ak=A_concat.stride(0),\n            stride_bk=self.stride_bk,\n            stride_bn=self.stride_bn,\n            stride_ck=C.stride(0),\n            total_tiles=total_tiles,\n            BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n            GROUP_SIZE_M=8,\n            NUM_GROUPS=num_groups\n        )\n        \n        return C\n\ndef test_grouped_gemm():\n    torch.manual_seed(0)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cpu\":\n        print(\"Skipping test on CPU\")\n        return\n\n    # Setup dimensions\n    num_groups = 4\n    K = 128\n    M_splits = [64, 128, 32, 256]\n    Ns = [128, 256, 128, 64]\n    \n    # Create B tensors (Weights)\n    # Simulating FP8 by creating float16 and casting/handling conceptually\n    # Note: Real FP8 creation requires specific hardware or casting utilities.\n    # Here we use float16 for 'data' but the kernel treats it as FP8 bits.\n    B_list = [torch.randn(K, n, device=device, dtype=torch.float16) for n in Ns]\n    \n    # Create A input\n    total_M = sum(M_splits)\n    A_concat = torch.randn(total_M, K, device=device, dtype=torch.float16)\n    \n    # Instantiate GEMM\n    gemm_op = OptimizedGroupedGEMM(B_list, device=device)\n    \n    # Run\n    C_out = gemm_op(A_concat, M_splits)\n    \n    print(f\"Output shape: {C_out.shape}\")\n    print(\"Kernel executed successfully.\")\n\nif __name__ == \"__main__\":\n    test_grouped_gemm()\n", "speedup": 0.0}

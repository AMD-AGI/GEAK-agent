{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton-accelerated function embedding_kernel is specialized for extracting and storing embedding vectors from a weight matrix for a sequence of token IDs. It uses program IDs to determine processing offsets and handles iteration over sequences with BLOCK_N and BLOCK_NN stride sizes. For each sequence, it computes token IDs and uses masks to ensure only valid data is loaded and processed. The weight matrix is addressed using a combination of token IDs and dimension offsets, facilitated by the stride of the weight tensor. The processed vectors are then stored into the 'out' tensor using calculated strides and masks, ensuring each output sequence position receives the correct embedding vector. The wrapping function, embedding, configures and invokes the kernel with appropriate grid settings, aligning the number of warps and stages for optimal performance.\n            ", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    start_n = tl.program_id(0) * BLOCK_N\n\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq + offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :], vecs, mask=store_mask)\n\n@torch.no_grad()\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    BLOCK_N = 64\n    BLOCK_NN = 1\n    BLOCK_DMODEL = triton.next_power_of_2(weight.shape[1])\n    n_ctx = input_ids.shape[0]\n\n    grid = (triton.cdiv(n_ctx, BLOCK_N), 1, 1)\n\n    embedding_kernel[grid](\n        weight,\n        input_ids,\n        out,\n        vob_start_id,\n        vob_end_id,\n        weight.stride(0),\n        out.stride(0),\n        n_ctx=n_ctx,\n        hiden_size=weight.shape[1],\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK_N,\n        BLOCK_NN=BLOCK_NN,\n        num_warps=1,\n        num_stages=1,\n    )\n\n\n\n", "filename": "embedding_triton_kernel.py", "test_code": "import torch\n\ndef test_embedding():\n    # \u53c2\u6570\u5b9a\u4e49\n    vocab_size = 1000         # \u8bcd\u6c47\u8868\u5927\u5c0f\n    embedding_dim = 512       # \u5d4c\u5165\u7ef4\u5ea6\n    sequence_length = 128     # \u8f93\u5165\u5e8f\u5217\u957f\u5ea6\n    vob_start_id = 10         # \u8bcd\u6c47\u8868\u8d77\u59cb ID\n    vob_end_id = 1000         # \u8bcd\u6c47\u8868\u7ed3\u675f ID\n\n    # \u521b\u5efa\u6d4b\u8bd5\u8f93\u5165\u5f20\u91cf\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n\n    # \u8c03\u7528\u5d4c\u5165\u51fd\u6570\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n\n    # \u4fdd\u5b58\u7ed3\u679c\n    results = {}\n    results['test_case_1'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u8f93\u5165\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_2'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u8bcd\u6c47\u8868\u8303\u56f4\n    vob_start_id = 0\n    vob_end_id = 500\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_3'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u5d4c\u5165\u7ef4\u5ea6\n    embedding_dim = 256\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_4'] = out.clone()\n\n    return results\n\nresult_gold = test_embedding()", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 64, 'BLOCK_DMODEL': 64, 'BLOCK_NN': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 64, 'BLOCK_DMODEL': 128, 'BLOCK_NN': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 64, 'BLOCK_DMODEL': 256, 'BLOCK_NN': 64}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 64, 'BLOCK_DMODEL': 512, 'BLOCK_NN': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 64, 'BLOCK_DMODEL': 1024, 'BLOCK_NN': 64}, num_warps=8, num_stages=2),\n        triton.Config({'BLOCK_N': 32, 'BLOCK_DMODEL': 256, 'BLOCK_NN': 32}, num_warps=4, num_stages=2),\n        triton.Config({'BLOCK_N': 32, 'BLOCK_DMODEL': 512, 'BLOCK_NN': 32}, num_warps=8, num_stages=2),\n    ],\n    key=['n_ctx', 'hiden_size'],\n)\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    # 2D Grid Parallelism\n    # pid(0) -> Sequence Block\n    # pid(1) -> Hidden Dimension Block\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n\n    # 1. Sequence Dimension Offsets\n    # Process BLOCK_N tokens per program instance\n    offs_m = pid_m * BLOCK_N + tl.arange(0, BLOCK_N)\n    mask_m = offs_m < n_ctx\n\n    # 2. Hidden Dimension Offsets\n    # Process BLOCK_DMODEL hidden units\n    offs_k = pid_k * BLOCK_DMODEL + tl.arange(0, BLOCK_DMODEL)\n    mask_k = offs_k < hiden_size\n\n    # 3. Load Input IDs\n    # Access input_ids as a flattened linear sequence\n    input_ids_ptr = input_ids + offs_m\n    ids = tl.load(input_ids_ptr, mask=mask_m, other=0)\n\n    # 4. Vocabulary Shard Validation\n    # Check if IDs fall within the current vocabulary shard [vob_start_id, vob_end_id)\n    valid_id_mask = (ids >= vob_start_id) & (ids < vob_end_id)\n\n    # 5. Safe Pointer Arithmetic\n    # Clamp IDs to vob_start_id where invalid. This ensures that the pointer calculation\n    # (weight + row * stride) always results in a valid memory address within the weight tensor,\n    # preventing segmentation faults or OOB accesses before the mask is applied.\n    safe_ids = tl.where(valid_id_mask, ids, vob_start_id)\n    local_ids = safe_ids - vob_start_id\n\n    # 6. Weight Address Calculation\n    # Address = weight_base + (row_idx * stride_weight_seq) + col_idx\n    # Broadcast local_ids [BLOCK_N, 1] against offs_k [1, BLOCK_DMODEL]\n    weight_ptrs = weight + (local_ids[:, None] * stride_weight_seq) + offs_k[None, :]\n\n    # 7. Load Embedding Vectors\n    # Load Mask: Valid Sequence AND Valid Vocab ID AND Valid Hidden Dim\n    load_mask = mask_m[:, None] & valid_id_mask[:, None] & mask_k[None, :]\n    # We use 0.0 for 'other' so invalid/OOB tokens result in zero vectors\n    weight_vals = tl.load(weight_ptrs, mask=load_mask, other=0.0)\n\n    # 8. Store to Output\n    # Output Address = out_base + (seq_idx * stride_out_seq) + col_idx\n    out_ptrs = out + (offs_m[:, None] * stride_out_seq) + offs_k[None, :]\n    \n    # Store Mask: Valid Sequence AND Valid Hidden Dim\n    store_mask = mask_m[:, None] & mask_k[None, :]\n    tl.store(out_ptrs, weight_vals, mask=store_mask)\n\n\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    # Flatten input_ids to simplify kernel to a linear sequence of tokens\n    input_ids_flat = input_ids.view(-1)\n    \n    # Ensure memory contiguity for safe pointer arithmetic inside the kernel\n    if not input_ids_flat.is_contiguous():\n        input_ids_flat = input_ids_flat.contiguous()\n    if not out.is_contiguous():\n        out = out.contiguous()\n\n    n_ctx = input_ids_flat.numel()\n    hiden_size = weight.shape[1]\n\n    # Calculate Strides\n    stride_weight_seq = weight.stride(0)\n    # Calculate output stride relative to the flattened sequence dimension.\n    # View as (-1, hiden_size) to ensure dim 0 stride corresponds to stepping to the next token.\n    stride_out_seq = out.view(-1, hiden_size).stride(0)\n\n    # Define Grid\n    # The autotuner will provide BLOCK_N and BLOCK_DMODEL via the META dictionary.\n    # We utilize a 2D grid to parallelize over both sequence and hidden dimensions.\n    grid = lambda META: (\n        triton.cdiv(n_ctx, META['BLOCK_N']),\n        triton.cdiv(hiden_size, META['BLOCK_DMODEL'])\n    )\n\n    # Launch Kernel\n    # Note: BLOCK_N, BLOCK_DMODEL, and BLOCK_NN are injected by the autotuner configuration.\n    embedding_kernel[grid](\n        weight,\n        input_ids_flat,\n        out,\n        vob_start_id,\n        vob_end_id,\n        stride_weight_seq,\n        stride_out_seq,\n        n_ctx,\n        hiden_size=hiden_size,\n    )\n    \n    return out", "speedup": 4.257}

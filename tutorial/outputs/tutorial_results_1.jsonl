{"instruction": "Develop an optimized Triton kernel for Fused Grouped FP8 GEMM aimed at Mixture-of-Experts (MoE) workloads. The kernel must support variable-sized groups (Ragged Batching) where multiple GEMM operations share weights (B) but have different input M dimensions (A). Key requirements include: 1) FP8 support (E4M3) with per-tensor scaling factors for quantization/dequantization, 2) Grouped GEMM scheduling where 'tile_to_group_ptr' maps every tile to its group ID for load balancing, 3) Use a pointer-array ('B_ptr_array') for non-contiguous weight matrices, 4) Wrap host logic in a class 'OptimizedGroupedGEMM', 5) Include a 'test_grouped_gemm' function and 'if __name__ == \"__main__\":' block. CRITICAL CONSTRAINTS: 6) Use standard pointer arithmetic (tl.load with offsets). Do NOT use 'tl.make_block_ptr'. 7) DO NOT define or call any helper functions (like 'get_fp8_dtype') inside the JIT kernel. 8) DO NOT import 'transformer_engine'. Use only 'torch' and 'triton'. 9) For the FP8 data type, you MUST use 'tl.float8e4b8' directly. Do not use 'tl.float8e4m3fn'. 10) The kernel's output should be bfloat16. 11) When performing FP8 matrix multiplication using tl.dot, explicitly cast both input operands to the target FP8 data type (e.g., tl.float8e4b8) immediately before the dot operation to prevent data type errors.", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n    ],\n    key=['total_tiles', 'K'],\n)\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    group_id = tl.load(tile_to_group_ptr + pid)\n    local_tile_id = tl.load(tile_offset_ptr + pid)\n    \n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n    m_offset = tl.load(M_cumsum_ptr + group_id)\n    \n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id_tile = local_tile_id // num_pid_in_group\n    first_pid_m = group_id_tile * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (local_tile_id % group_size_m)\n    pid_n = (local_tile_id % num_pid_in_group) // group_size_m\n    \n    scale_base = group_id * 4\n    qscale_a = tl.load(scales_ptr + scale_base + 0)\n    qscale_b = tl.load(scales_ptr + scale_base + 1)\n    dscale_a = tl.load(scales_ptr + scale_base + 2)\n    dscale_b = tl.load(scales_ptr + scale_base + 3)\n    \n    b_ptr = tl.load(B_ptr_array + group_id).to(tl.pointer_type(tl.bfloat16))\n    \n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = A_ptr + (m_offset + offs_am[:, None]) * stride_ak + offs_k[None, :]\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        \n        # --- M/N/K Masking ---\n        # A mask: check M bounds (rows) and K bounds (cols)\n        k_mask_a = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        \n        # B mask: check N bounds (cols) and K bounds (rows)\n        k_mask_b = (offs_bn[None, :] < N) & (offs_k[:, None] < k_remaining)\n        \n        a_bf16 = tl.load(a_ptrs, mask=k_mask_a, other=0.0)\n        b_bf16 = tl.load(b_ptrs, mask=k_mask_b, other=0.0)\n        \n        a_fp8 = (a_bf16 * qscale_a).to(tl.float8e4b8)\n        b_fp8 = (b_bf16 * qscale_b).to(tl.float8e4b8)\n        \n        accumulator = tl.dot(a_fp8, b_fp8, accumulator)\n        \n        a_ptrs += BLOCK_K\n        b_ptrs += BLOCK_K * stride_bk\n    \n    total_dscale = dscale_a * dscale_b\n    c = accumulator * total_dscale\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = C_ptr + (m_offset + offs_cm[:, None]) * stride_ck + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    \n    tl.store(c_ptrs, c.to(tl.bfloat16), mask=c_mask)\n\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        self.num_groups = len(B_list)\n        self.K = B_list[0].shape[0]\n        self.N = B_list[0].shape[1]\n        self.FP8_MAX = 240.0\n        \n        self.B_list = [b.contiguous() for b in B_list]\n        self.B_ptr_array = torch.tensor(\n            [b.data_ptr() for b in self.B_list],\n            device=device, dtype=torch.int64\n        )\n        \n        self.B_qscales = torch.empty(self.num_groups, device=device, dtype=torch.float32)\n        self.B_dscales = torch.empty(self.num_groups, device=device, dtype=torch.float32)\n        for i, B in enumerate(self.B_list):\n            max_b = B.abs().max().float().item()\n            max_b = max(max_b, 1e-6)\n            self.B_qscales[i] = self.FP8_MAX / max_b\n            self.B_dscales[i] = max_b / self.FP8_MAX\n        \n        self.scales_buffer = torch.empty(self.num_groups * 4, device=device, dtype=torch.float32)\n        for i in range(self.num_groups):\n            self.scales_buffer[i*4 + 1] = self.B_qscales[i]\n            self.scales_buffer[i*4 + 3] = self.B_dscales[i]\n        \n        self._cached_m_splits = None\n        self._cached_schedule = None\n        self._cached_C_out = None\n        \n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        M_cumsum = [0]\n        tile_to_group = []\n        tile_offset = []\n        M_array = []\n        N_array = []\n        \n        for group_id, M in enumerate(M_splits):\n            M_cumsum.append(M_cumsum[-1] + M)\n            M_array.append(M)\n            N_array.append(self.N)\n            \n            num_tiles_m = triton.cdiv(M, BLOCK_M)\n            num_tiles_n = triton.cdiv(self.N, BLOCK_N)\n            \n            for local_id in range(num_tiles_m * num_tiles_n):\n                tile_to_group.append(group_id)\n                tile_offset.append(local_id)\n        \n        return {\n            'M_cumsum': torch.tensor(M_cumsum[:-1], device=self.device, dtype=torch.int32),\n            'M_array': torch.tensor(M_array, device=self.device, dtype=torch.int32),\n            'N_array': torch.tensor(N_array, device=self.device, dtype=torch.int32),\n            'tile_to_group': torch.tensor(tile_to_group, device=self.device, dtype=torch.int32),\n            'tile_offset': torch.tensor(tile_offset, device=self.device, dtype=torch.int32),\n            'total_tiles': len(tile_to_group),\n            'total_M': sum(M_splits),\n        }\n    \n    def _get_schedule(self, M_splits: List[int]):\n        m_tuple = tuple(M_splits)\n        if self._cached_m_splits != m_tuple:\n            self._cached_schedule = self._compute_schedule(M_splits)\n            self._cached_m_splits = m_tuple\n            total_M = self._cached_schedule['total_M']\n            self._cached_C_out = torch.empty((total_M, self.N), device=self.device, dtype=torch.bfloat16)\n        return self._cached_schedule\n    \n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]) -> torch.Tensor:\n        schedule = self._get_schedule(M_splits)\n        \n        start = 0\n        for i, m in enumerate(M_splits):\n            max_a = A_concat[start:start+m].abs().max().float().item()\n            max_a = max(max_a, 1e-6)\n            qscale_a = self.FP8_MAX / max_a\n            self.scales_buffer[i*4 + 0] = qscale_a\n            self.scales_buffer[i*4 + 2] = 1.0 / qscale_a\n            start += m\n        \n        grid = (schedule['total_tiles'],)\n        \n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_concat, self._cached_C_out,\n            self.B_ptr_array,\n            schedule['M_cumsum'],\n            schedule['M_array'], schedule['N_array'], self.K,\n            self.scales_buffer,\n            schedule['tile_to_group'], schedule['tile_offset'],\n            A_concat.stride(0), self.B_list[0].stride(0), self.B_list[0].stride(1), \n            self._cached_C_out.stride(0),\n            schedule['total_tiles'],\n            NUM_GROUPS=self.num_groups,\n        )\n        \n        return self._cached_C_out\n", "filename": "grouped_gemm_triton_kernel.py", "predict": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List, Optional\n\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    # Program ID\n    pid = tl.program_id(axis=0)\n    if pid >= total_tiles:\n        return\n\n    # 1. Determine which group (expert) and which tile within that group this PID handles\n    group_id = tl.load(tile_to_group_ptr + pid)\n    tile_flat_idx = tl.load(tile_offset_ptr + pid)\n\n    # 2. Load metadata for this group\n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n    M_start = tl.load(M_cumsum_ptr + group_id)\n\n    # 3. Calculate block indices (m, n) relative to the group\n    num_n_blocks = tl.cdiv(N, BLOCK_N)\n    m_block = tile_flat_idx // num_n_blocks\n    n_block = tile_flat_idx % num_n_blocks\n\n    # 4. Load Scales (A_scale, B_scale)\n    # Assuming layout: [scale_a_0, scale_b_0, scale_a_1, scale_b_1, ...]\n    scale_a = tl.load(scales_ptr + group_id * 2)\n    scale_b = tl.load(scales_ptr + group_id * 2 + 1)\n\n    # 5. Calculate Pointers\n    \n    # A Pointer Setup\n    # A is (Total_M, K) Row Major. Stride M is K, Stride K is stride_ak (usually 1)\n    offs_am = M_start + m_block * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_K)\n    # Base A pointer for this block\n    # Note: A_ptr is base. Row stride is K. Col stride is stride_ak.\n    a_ptrs = A_ptr + (offs_am[:, None] * K + offs_k[None, :] * stride_ak)\n\n    # B Pointer Setup via Pointer Array\n    # Load the base pointer for B[group_id] as int64\n    b_ptr_int = tl.load(B_ptr_array + group_id)\n    # Cast to pointer type\n    b_base_ptr = b_ptr_int.to(tl.pointer_type(tl.float8e4b8))\n    \n    offs_bn = n_block * BLOCK_N + tl.arange(0, BLOCK_N)\n    # B ptrs: base + k * stride_bk + n * stride_bn\n    b_ptrs = b_base_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # 6. Main Loop\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_curr = k * BLOCK_K\n        \n        # Load A\n        # Mask: Check if row is within M bounds of this group\n        a_mask = (offs_am[:, None] < (M_start + M)) & ((k_curr + offs_k[None, :]) < K)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        \n        # Load B\n        b_mask = (offs_bn[None, :] < N) & ((k_curr + offs_k[:, None]) < K)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        # Cast to standard FP8 type for AMD/Triton\n        a_fp8 = a.to(tl.float8e4b8)\n        b_fp8 = b.to(tl.float8e4b8)\n        \n        # Compute Dot Product\n        acc += tl.dot(a_fp8, b_fp8)\n        \n        # Advance pointers\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    # 7. Epilogue\n    # Dequantize/Quantize to Output (bfloat16)\n    acc = acc * scale_a * scale_b\n    c = acc.to(tl.bfloat16)\n\n    # Store C\n    # C is (Total_M, N) Row Major usually, or packed similarly to A\n    # Using N from the group (assuming N is consistent or C is packed tightly)\n    # Stride M for C is N.\n    offs_cm = M_start + m_block * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = n_block * BLOCK_N + tl.arange(0, BLOCK_N)\n    \n    c_ptrs = C_ptr + (offs_cm[:, None] * N * stride_ck + offs_cn[None, :] * stride_ck)\n    c_mask = (offs_cm[:, None] < (M_start + M)) & (offs_cn[None, :] < N)\n    \n    tl.store(c_ptrs, c, mask=c_mask)\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        self.num_groups = len(B_list)\n        self.B_list = B_list\n        \n        # Pre-process B pointers\n        # B_ptr_array must be int64 for Triton to load\n        self.B_ptr_array = torch.tensor(\n            [b.data_ptr() for b in B_list], \n            dtype=torch.int64, \n            device=device\n        )\n        \n        # Extract dimensions from first B (assuming K is constant)\n        # B is (K, N) or (N, K). Assuming (K, N) RowMajor for simplicity or checking shape\n        self.K = B_list[0].shape[0]\n        self.N = B_list[0].shape[1]\n        \n        # Initialize scales (A_scale, B_scale per group). \n        # Initializing to 1.0. In real usage, these would be updated or passed.\n        self.scales = torch.ones((self.num_groups, 2), dtype=torch.float32, device=device)\n        \n        # Cache for schedules to avoid re-computing on host if possible\n        self.schedule_cache = {}\n\n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        tile_to_group_list = []\n        tile_offset_list = []\n        \n        for group_id, m in enumerate(M_splits):\n            # Calculate tiles needed for this group\n            num_m_tiles = (m + BLOCK_M - 1) // BLOCK_M\n            num_n_tiles = (self.N + BLOCK_N - 1) // BLOCK_N\n            num_tiles = num_m_tiles * num_n_tiles\n            \n            if num_tiles > 0:\n                tile_to_group_list.append(torch.full((num_tiles,), group_id, dtype=torch.int32, device=self.device))\n                tile_offset_list.append(torch.arange(num_tiles, dtype=torch.int32, device=self.device))\n        \n        if not tile_to_group_list:\n            return torch.empty(0, dtype=torch.int32, device=self.device), torch.empty(0, dtype=torch.int32, device=self.device)\n\n        tile_to_group = torch.cat(tile_to_group_list)\n        tile_offset = torch.cat(tile_offset_list)\n        return tile_to_group, tile_offset\n\n    def _get_schedule(self, M_splits: List[int]):\n        # Simple cache based on tuple of splits\n        key = tuple(M_splits)\n        if key not in self.schedule_cache:\n            self.schedule_cache[key] = self._compute_schedule(M_splits)\n        return self.schedule_cache[key]\n\n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]):\n        # Prepare metadata\n        M_tensor = torch.tensor(M_splits, dtype=torch.int32, device=self.device)\n        N_tensor = torch.full((self.num_groups,), self.N, dtype=torch.int32, device=self.device)\n        \n        # Compute cumsum for A offsets\n        M_cumsum = torch.zeros(self.num_groups + 1, dtype=torch.int32, device=self.device)\n        M_cumsum[1:] = torch.cumsum(M_tensor, dim=0)\n        M_cumsum_ptr = M_cumsum[:self.num_groups]\n        \n        # Get schedule\n        tile_to_group, tile_offset = self._get_schedule(M_splits)\n        total_tiles = tile_to_group.numel()\n        \n        # Output buffer\n        total_M = sum(M_splits)\n        C = torch.empty((total_M, self.N), dtype=torch.bfloat16, device=self.device)\n        \n        # Strides\n        stride_ak = 1 # Assuming A is RowMajor (M, K) with contiguous rows\n        # For B, assuming B_list tensors are contiguous RowMajor (K, N)\n        # stride_bk = N, stride_bn = 1\n        stride_bk = self.N\n        stride_bn = 1\n        stride_ck = 1 # C is RowMajor (M, N)\n        \n        BLOCK_M = 128\n        BLOCK_N = 128\n        BLOCK_K = 128\n        GROUP_SIZE_M = 8\n        \n        grid = (total_tiles,)\n        \n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_ptr=A_concat,\n            C_ptr=C,\n            B_ptr_array=self.B_ptr_array,\n            M_cumsum_ptr=M_cumsum_ptr,\n            M_array=M_tensor,\n            N_array=N_tensor,\n            K=self.K,\n            scales_ptr=self.scales,\n            tile_to_group_ptr=tile_to_group,\n            tile_offset_ptr=tile_offset,\n            stride_ak=stride_ak,\n            stride_bk=stride_bk,\n            stride_bn=stride_bn,\n            stride_ck=stride_ck,\n            total_tiles=total_tiles,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n            BLOCK_K=BLOCK_K,\n            GROUP_SIZE_M=GROUP_SIZE_M,\n            NUM_GROUPS=self.num_groups\n        )\n        \n        return C\n\ndef test_grouped_gemm():\n    torch.manual_seed(0)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cpu\":\n        print(\"Skipping test on CPU\")\n        return\n\n    # Dimensions\n    num_groups = 4\n    K = 128\n    N = 128\n    M_splits = [64, 128, 32, 256]\n    total_M = sum(M_splits)\n\n    # Create inputs\n    # A: FP8 (simulated as float8_e4m3fn or e5m2, but for logic we pass tensor)\n    # Note: Torch doesn't fully support e4m3/b8 arithmetic on all HW yet, \n    # but we can allocate it.\n    # Using float8_e4m3fn for storage simulation.\n    try:\n        dtype_fp8 = torch.float8_e4m3fn\n    except:\n        # Fallback if torch version is old, though script requires modern torch\n        print(\"Torch FP8 not supported, skipping execution\")\n        return\n\n    A = torch.randn(total_M, K, device=device, dtype=torch.float32).to(dtype_fp8)\n    \n    B_list = []\n    for _ in range(num_groups):\n        b = torch.randn(K, N, device=device, dtype=torch.float32).to(dtype_fp8)\n        B_list.append(b)\n\n    gemm_op = OptimizedGroupedGEMM(B_list, device=device)\n    \n    # Run kernel\n    C_out = gemm_op(A, M_splits)\n    \n    print(f\"Kernel executed successfully. Output shape: {C_out.shape}\")\n    assert C_out.shape == (total_M, N)\n    assert C_out.dtype == torch.bfloat16\n\nif __name__ == \"__main__\":\n    test_grouped_gemm()", "speedup": 0.0}

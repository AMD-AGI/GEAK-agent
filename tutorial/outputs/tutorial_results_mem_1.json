{"grouped_gemm_triton_kernel.py": {"call_err_msg": "None", "exe_err_msg": "Generated output is None", "reflection": null, "oneshot": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_moe_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    a_scale_ptr,\n    b_scale_ptr,\n    topk_weights_ptr,\n    sorted_token_ids_ptr,\n    expert_ids_ptr,\n    num_tokens_post_padded_ptr,\n    N,\n    K,\n    EM,\n    num_valid_tokens,\n    stride_am,\n    stride_ak,\n    stride_be,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    MUL_ROUTED_WEIGHT: tl.constexpr,\n    top_k: tl.constexpr,\n    compute_type: tl.constexpr,\n    use_fp8: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am +\n                      offs_k[None, :] * stride_ak)\n\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n                                                offs_bn[None, :] * stride_bn)\n\n    if use_fp8:\n        a_scale = tl.load(a_scale_ptr)\n        b_scale = tl.load(b_scale_ptr + off_experts)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs,\n                    mask=token_mask[:, None] &\n                    (offs_k[None, :] < K - k * BLOCK_SIZE_K),\n                    other=0.0)\n        b = tl.load(b_ptrs,\n                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n                    other=0.0)\n        if use_fp8:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        else:\n            accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token,\n                             mask=token_mask,\n                             other=0)\n        accumulator = accumulator * moe_weight[:, None]\n\n    if use_fp8:\n        accumulator = (accumulator * a_scale * b_scale).to(compute_type)\n    else:\n        accumulator = accumulator.to(compute_type)\n\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n\ndef invoke_fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor,\n                            A_scale: Optional[torch.Tensor],\n                            B_scale: Optional[torch.Tensor],\n                            topk_weights: torch.Tensor, topk_ids: torch.Tensor,\n                            sorted_token_ids: torch.Tensor,\n                            expert_ids: torch.Tensor,\n                            num_tokens_post_padded: torch.Tensor,\n                            mul_routed_weight: bool, top_k: int,\n                            config: Dict[str, Any], compute_type: tl.dtype,\n                            use_fp8: bool) -> None:\n    grid = lambda META: (triton.cdiv(sorted_token_ids.shape[0], META[\n        'BLOCK_SIZE_M']) * triton.cdiv(B.shape[1], META['BLOCK_SIZE_N']), )\n\n    fused_moe_kernel[grid](\n        A,\n        B,\n        C,\n        A_scale,\n        B_scale,\n        topk_weights,\n        sorted_token_ids,\n        expert_ids,\n        num_tokens_post_padded,\n        B.shape[1],\n        B.shape[2],\n        sorted_token_ids.shape[0],\n        topk_ids.numel(),\n        A.stride(0),\n        A.stride(1),\n        B.stride(0),\n        B.stride(2),\n        B.stride(1),\n        C.stride(1),\n        C.stride(2),\n        MUL_ROUTED_WEIGHT=mul_routed_weight,\n        top_k=top_k,\n        compute_type=compute_type,\n        use_fp8=use_fp8,\n        **config,\n    )\n", "perf_candidates": [], "perf_strategy": null, "call_candidate": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List, Optional\n\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    # Program ID\n    pid = tl.program_id(axis=0)\n    if pid >= total_tiles:\n        return\n\n    # 1. Determine which group (expert) and which tile within that group this PID handles\n    group_id = tl.load(tile_to_group_ptr + pid)\n    tile_flat_idx = tl.load(tile_offset_ptr + pid)\n\n    # 2. Load metadata for this group\n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n    M_start = tl.load(M_cumsum_ptr + group_id)\n\n    # 3. Calculate block indices (m, n) relative to the group\n    num_n_blocks = tl.cdiv(N, BLOCK_N)\n    m_block = tile_flat_idx // num_n_blocks\n    n_block = tile_flat_idx % num_n_blocks\n\n    # 4. Load Scales (A_scale, B_scale)\n    # Assuming layout: [scale_a_0, scale_b_0, scale_a_1, scale_b_1, ...]\n    scale_a = tl.load(scales_ptr + group_id * 2)\n    scale_b = tl.load(scales_ptr + group_id * 2 + 1)\n\n    # 5. Calculate Pointers\n    \n    # A Pointer Setup\n    # A is (Total_M, K) Row Major. Stride M is K, Stride K is stride_ak (usually 1)\n    offs_am = M_start + m_block * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_K)\n    # Base A pointer for this block\n    # Note: A_ptr is base. Row stride is K. Col stride is stride_ak.\n    a_ptrs = A_ptr + (offs_am[:, None] * K + offs_k[None, :] * stride_ak)\n\n    # B Pointer Setup via Pointer Array\n    # Load the base pointer for B[group_id] as int64\n    b_ptr_int = tl.load(B_ptr_array + group_id)\n    # Cast to pointer type\n    b_base_ptr = b_ptr_int.to(tl.pointer_type(tl.float8e4b8))\n    \n    offs_bn = n_block * BLOCK_N + tl.arange(0, BLOCK_N)\n    # B ptrs: base + k * stride_bk + n * stride_bn\n    b_ptrs = b_base_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # 6. Main Loop\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_curr = k * BLOCK_K\n        \n        # Load A\n        # Mask: Check if row is within M bounds of this group\n        a_mask = (offs_am[:, None] < (M_start + M)) & ((k_curr + offs_k[None, :]) < K)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        \n        # Load B\n        b_mask = (offs_bn[None, :] < N) & ((k_curr + offs_k[:, None]) < K)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        \n        # Cast to standard FP8 type for AMD/Triton\n        a_fp8 = a.to(tl.float8e4b8)\n        b_fp8 = b.to(tl.float8e4b8)\n        \n        # Compute Dot Product\n        acc += tl.dot(a_fp8, b_fp8)\n        \n        # Advance pointers\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    # 7. Epilogue\n    # Dequantize/Quantize to Output (bfloat16)\n    acc = acc * scale_a * scale_b\n    c = acc.to(tl.bfloat16)\n\n    # Store C\n    # C is (Total_M, N) Row Major usually, or packed similarly to A\n    # Using N from the group (assuming N is consistent or C is packed tightly)\n    # Stride M for C is N.\n    offs_cm = M_start + m_block * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = n_block * BLOCK_N + tl.arange(0, BLOCK_N)\n    \n    c_ptrs = C_ptr + (offs_cm[:, None] * N * stride_ck + offs_cn[None, :] * stride_ck)\n    c_mask = (offs_cm[:, None] < (M_start + M)) & (offs_cn[None, :] < N)\n    \n    tl.store(c_ptrs, c, mask=c_mask)\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        self.num_groups = len(B_list)\n        self.B_list = B_list\n        \n        # Pre-process B pointers\n        # B_ptr_array must be int64 for Triton to load\n        self.B_ptr_array = torch.tensor(\n            [b.data_ptr() for b in B_list], \n            dtype=torch.int64, \n            device=device\n        )\n        \n        # Extract dimensions from first B (assuming K is constant)\n        # B is (K, N) or (N, K). Assuming (K, N) RowMajor for simplicity or checking shape\n        self.K = B_list[0].shape[0]\n        self.N = B_list[0].shape[1]\n        \n        # Initialize scales (A_scale, B_scale per group). \n        # Initializing to 1.0. In real usage, these would be updated or passed.\n        self.scales = torch.ones((self.num_groups, 2), dtype=torch.float32, device=device)\n        \n        # Cache for schedules to avoid re-computing on host if possible\n        self.schedule_cache = {}\n\n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        tile_to_group_list = []\n        tile_offset_list = []\n        \n        for group_id, m in enumerate(M_splits):\n            # Calculate tiles needed for this group\n            num_m_tiles = (m + BLOCK_M - 1) // BLOCK_M\n            num_n_tiles = (self.N + BLOCK_N - 1) // BLOCK_N\n            num_tiles = num_m_tiles * num_n_tiles\n            \n            if num_tiles > 0:\n                tile_to_group_list.append(torch.full((num_tiles,), group_id, dtype=torch.int32, device=self.device))\n                tile_offset_list.append(torch.arange(num_tiles, dtype=torch.int32, device=self.device))\n        \n        if not tile_to_group_list:\n            return torch.empty(0, dtype=torch.int32, device=self.device), torch.empty(0, dtype=torch.int32, device=self.device)\n\n        tile_to_group = torch.cat(tile_to_group_list)\n        tile_offset = torch.cat(tile_offset_list)\n        return tile_to_group, tile_offset\n\n    def _get_schedule(self, M_splits: List[int]):\n        # Simple cache based on tuple of splits\n        key = tuple(M_splits)\n        if key not in self.schedule_cache:\n            self.schedule_cache[key] = self._compute_schedule(M_splits)\n        return self.schedule_cache[key]\n\n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]):\n        # Prepare metadata\n        M_tensor = torch.tensor(M_splits, dtype=torch.int32, device=self.device)\n        N_tensor = torch.full((self.num_groups,), self.N, dtype=torch.int32, device=self.device)\n        \n        # Compute cumsum for A offsets\n        M_cumsum = torch.zeros(self.num_groups + 1, dtype=torch.int32, device=self.device)\n        M_cumsum[1:] = torch.cumsum(M_tensor, dim=0)\n        M_cumsum_ptr = M_cumsum[:self.num_groups]\n        \n        # Get schedule\n        tile_to_group, tile_offset = self._get_schedule(M_splits)\n        total_tiles = tile_to_group.numel()\n        \n        # Output buffer\n        total_M = sum(M_splits)\n        C = torch.empty((total_M, self.N), dtype=torch.bfloat16, device=self.device)\n        \n        # Strides\n        stride_ak = 1 # Assuming A is RowMajor (M, K) with contiguous rows\n        # For B, assuming B_list tensors are contiguous RowMajor (K, N)\n        # stride_bk = N, stride_bn = 1\n        stride_bk = self.N\n        stride_bn = 1\n        stride_ck = 1 # C is RowMajor (M, N)\n        \n        BLOCK_M = 128\n        BLOCK_N = 128\n        BLOCK_K = 128\n        GROUP_SIZE_M = 8\n        \n        grid = (total_tiles,)\n        \n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_ptr=A_concat,\n            C_ptr=C,\n            B_ptr_array=self.B_ptr_array,\n            M_cumsum_ptr=M_cumsum_ptr,\n            M_array=M_tensor,\n            N_array=N_tensor,\n            K=self.K,\n            scales_ptr=self.scales,\n            tile_to_group_ptr=tile_to_group,\n            tile_offset_ptr=tile_offset,\n            stride_ak=stride_ak,\n            stride_bk=stride_bk,\n            stride_bn=stride_bn,\n            stride_ck=stride_ck,\n            total_tiles=total_tiles,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n            BLOCK_K=BLOCK_K,\n            GROUP_SIZE_M=GROUP_SIZE_M,\n            NUM_GROUPS=self.num_groups\n        )\n        \n        return C\n\ndef test_grouped_gemm():\n    torch.manual_seed(0)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cpu\":\n        print(\"Skipping test on CPU\")\n        return\n\n    # Dimensions\n    num_groups = 4\n    K = 128\n    N = 128\n    M_splits = [64, 128, 32, 256]\n    total_M = sum(M_splits)\n\n    # Create inputs\n    # A: FP8 (simulated as float8_e4m3fn or e5m2, but for logic we pass tensor)\n    # Note: Torch doesn't fully support e4m3/b8 arithmetic on all HW yet, \n    # but we can allocate it.\n    # Using float8_e4m3fn for storage simulation.\n    try:\n        dtype_fp8 = torch.float8_e4m3fn\n    except:\n        # Fallback if torch version is old, though script requires modern torch\n        print(\"Torch FP8 not supported, skipping execution\")\n        return\n\n    A = torch.randn(total_M, K, device=device, dtype=torch.float32).to(dtype_fp8)\n    \n    B_list = []\n    for _ in range(num_groups):\n        b = torch.randn(K, N, device=device, dtype=torch.float32).to(dtype_fp8)\n        B_list.append(b)\n\n    gemm_op = OptimizedGroupedGEMM(B_list, device=device)\n    \n    # Run kernel\n    C_out = gemm_op(A, M_splits)\n    \n    print(f\"Kernel executed successfully. Output shape: {C_out.shape}\")\n    assert C_out.shape == (total_M, N)\n    assert C_out.dtype == torch.bfloat16\n\nif __name__ == \"__main__\":\n    test_grouped_gemm()", "exe_candidate": null, "temp_strategy": "The implementation targets AMD ROCm using Triton. The kernel `grouped_fused_fp8_gemm_kernel_v2` is designed to handle ragged batching for MoE workloads. Key optimization strategies include: 1) **Ragged Scheduling**: We compute a schedule on the host that maps every global tile ID to a specific group (expert) and a relative tile index. This ensures perfect load balancing across compute units (CUs) even when experts have vastly different token counts. 2) **Pointer Arithmetics for B**: Since weights (B) are distinct per group and non-contiguous, we use a `B_ptr_array` (pointer to pointers) and manually cast integer addresses to pointers inside the kernel using `.to(tl.pointer_type(...))`. 3) **FP8 Math**: Inputs are explicitly cast to `tl.float8e4b8` (E4M3) right before `tl.dot` to match the hardware instructions, with accumulation in float32. 4) **Memory Layout**: We assume row-major layouts for A and C, calculating strides dynamically based on dimensions K and N, while B's strides are fully parameterized. 5) **Host Wrapper**: The `OptimizedGroupedGEMM` class pre-calculates static data (weights, scales) and efficiently manages dynamic scheduling metadata (M_splits) during the forward pass.", "perf_debug_num": 0, "pass_call": true, "pass_exe": true, "pass_perf": false}}
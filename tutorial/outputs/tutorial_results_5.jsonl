{"instruction": "Develop an optimized Triton kernel for Fused Grouped FP8 GEMM aimed at Mixture-of-Experts (MoE) workloads. The kernel must support variable-sized groups (Ragged Batching) where multiple GEMM operations share weights (B) but have different input M dimensions (A). Key requirements include: 1) FP8 support (E4M3) with per-tensor scaling factors for quantization/dequantization, 2) Grouped GEMM scheduling where 'tile_to_group_ptr' maps every tile to its group ID for load balancing, 3) Use a pointer-array ('B_ptr_array') for non-contiguous weight matrices, 4) Wrap host logic in a class 'OptimizedGroupedGEMM', 5) Include a 'test_grouped_gemm' function and 'if __name__ == \"__main__\":' block. CRITICAL CONSTRAINTS: 6) Use standard pointer arithmetic (tl.load with offsets). Do NOT use 'tl.make_block_ptr'. 7) DO NOT define or call any helper functions (like 'get_fp8_dtype') inside the JIT kernel. 8) DO NOT import 'transformer_engine'. Use only 'torch' and 'triton'. 9) For the FP8 data type, you MUST use 'tl.float8e4b8' directly. Do not use 'tl.float8e4m3fn'. 10) The kernel's output should be bfloat16. 11) When performing FP8 matrix multiplication using tl.dot, explicitly cast both input operands to the target FP8 data type (e.g., tl.float8e4b8) immediately before the dot operation to prevent data type errors.", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n    ],\n    key=['total_tiles', 'K'],\n)\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    \n    group_id = tl.load(tile_to_group_ptr + pid)\n    local_tile_id = tl.load(tile_offset_ptr + pid)\n    \n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n    m_offset = tl.load(M_cumsum_ptr + group_id)\n    \n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id_tile = local_tile_id // num_pid_in_group\n    first_pid_m = group_id_tile * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (local_tile_id % group_size_m)\n    pid_n = (local_tile_id % num_pid_in_group) // group_size_m\n    \n    scale_base = group_id * 4\n    qscale_a = tl.load(scales_ptr + scale_base + 0)\n    qscale_b = tl.load(scales_ptr + scale_base + 1)\n    dscale_a = tl.load(scales_ptr + scale_base + 2)\n    dscale_b = tl.load(scales_ptr + scale_base + 3)\n    \n    b_ptr = tl.load(B_ptr_array + group_id).to(tl.pointer_type(tl.bfloat16))\n    \n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n    \n    a_ptrs = A_ptr + (m_offset + offs_am[:, None]) * stride_ak + offs_k[None, :]\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n    \n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        k_remaining = K - k * BLOCK_K\n        \n        # --- M/N/K Masking ---\n        # A mask: check M bounds (rows) and K bounds (cols)\n        k_mask_a = (offs_am[:, None] < M) & (offs_k[None, :] < k_remaining)\n        \n        # B mask: check N bounds (cols) and K bounds (rows)\n        k_mask_b = (offs_bn[None, :] < N) & (offs_k[:, None] < k_remaining)\n        \n        a_bf16 = tl.load(a_ptrs, mask=k_mask_a, other=0.0)\n        b_bf16 = tl.load(b_ptrs, mask=k_mask_b, other=0.0)\n        \n        a_fp8 = (a_bf16 * qscale_a).to(tl.float8e4b8)\n        b_fp8 = (b_bf16 * qscale_b).to(tl.float8e4b8)\n        \n        accumulator = tl.dot(a_fp8, b_fp8, accumulator)\n        \n        a_ptrs += BLOCK_K\n        b_ptrs += BLOCK_K * stride_bk\n    \n    total_dscale = dscale_a * dscale_b\n    c = accumulator * total_dscale\n    \n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = C_ptr + (m_offset + offs_cm[:, None]) * stride_ck + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    \n    tl.store(c_ptrs, c.to(tl.bfloat16), mask=c_mask)\n\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        self.num_groups = len(B_list)\n        self.K = B_list[0].shape[0]\n        self.N = B_list[0].shape[1]\n        self.FP8_MAX = 240.0\n        \n        self.B_list = [b.contiguous() for b in B_list]\n        self.B_ptr_array = torch.tensor(\n            [b.data_ptr() for b in self.B_list],\n            device=device, dtype=torch.int64\n        )\n        \n        self.B_qscales = torch.empty(self.num_groups, device=device, dtype=torch.float32)\n        self.B_dscales = torch.empty(self.num_groups, device=device, dtype=torch.float32)\n        for i, B in enumerate(self.B_list):\n            max_b = B.abs().max().float().item()\n            max_b = max(max_b, 1e-6)\n            self.B_qscales[i] = self.FP8_MAX / max_b\n            self.B_dscales[i] = max_b / self.FP8_MAX\n        \n        self.scales_buffer = torch.empty(self.num_groups * 4, device=device, dtype=torch.float32)\n        for i in range(self.num_groups):\n            self.scales_buffer[i*4 + 1] = self.B_qscales[i]\n            self.scales_buffer[i*4 + 3] = self.B_dscales[i]\n        \n        self._cached_m_splits = None\n        self._cached_schedule = None\n        self._cached_C_out = None\n        \n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        M_cumsum = [0]\n        tile_to_group = []\n        tile_offset = []\n        M_array = []\n        N_array = []\n        \n        for group_id, M in enumerate(M_splits):\n            M_cumsum.append(M_cumsum[-1] + M)\n            M_array.append(M)\n            N_array.append(self.N)\n            \n            num_tiles_m = triton.cdiv(M, BLOCK_M)\n            num_tiles_n = triton.cdiv(self.N, BLOCK_N)\n            \n            for local_id in range(num_tiles_m * num_tiles_n):\n                tile_to_group.append(group_id)\n                tile_offset.append(local_id)\n        \n        return {\n            'M_cumsum': torch.tensor(M_cumsum[:-1], device=self.device, dtype=torch.int32),\n            'M_array': torch.tensor(M_array, device=self.device, dtype=torch.int32),\n            'N_array': torch.tensor(N_array, device=self.device, dtype=torch.int32),\n            'tile_to_group': torch.tensor(tile_to_group, device=self.device, dtype=torch.int32),\n            'tile_offset': torch.tensor(tile_offset, device=self.device, dtype=torch.int32),\n            'total_tiles': len(tile_to_group),\n            'total_M': sum(M_splits),\n        }\n    \n    def _get_schedule(self, M_splits: List[int]):\n        m_tuple = tuple(M_splits)\n        if self._cached_m_splits != m_tuple:\n            self._cached_schedule = self._compute_schedule(M_splits)\n            self._cached_m_splits = m_tuple\n            total_M = self._cached_schedule['total_M']\n            self._cached_C_out = torch.empty((total_M, self.N), device=self.device, dtype=torch.bfloat16)\n        return self._cached_schedule\n    \n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]) -> torch.Tensor:\n        schedule = self._get_schedule(M_splits)\n        \n        start = 0\n        for i, m in enumerate(M_splits):\n            max_a = A_concat[start:start+m].abs().max().float().item()\n            max_a = max(max_a, 1e-6)\n            qscale_a = self.FP8_MAX / max_a\n            self.scales_buffer[i*4 + 0] = qscale_a\n            self.scales_buffer[i*4 + 2] = 1.0 / qscale_a\n            start += m\n        \n        grid = (schedule['total_tiles'],)\n        \n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_concat, self._cached_C_out,\n            self.B_ptr_array,\n            schedule['M_cumsum'],\n            schedule['M_array'], schedule['N_array'], self.K,\n            self.scales_buffer,\n            schedule['tile_to_group'], schedule['tile_offset'],\n            A_concat.stride(0), self.B_list[0].stride(0), self.B_list[0].stride(1), \n            self._cached_C_out.stride(0),\n            schedule['total_tiles'],\n            NUM_GROUPS=self.num_groups,\n        )\n        \n        return self._cached_C_out\n", "filename": "grouped_gemm_triton_kernel.py", "predict": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import List\n\n@triton.jit\ndef grouped_fused_fp8_gemm_kernel_v2(\n    A_ptr, C_ptr,\n    B_ptr_array,\n    M_cumsum_ptr,\n    M_array, N_array, K,\n    scales_ptr,\n    tile_to_group_ptr, tile_offset_ptr,\n    stride_ak, stride_bk, stride_bn, stride_ck,\n    total_tiles,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    NUM_GROUPS: tl.constexpr,\n):\n    # 1. Map program ID to specific group and tile\n    pid = tl.program_id(axis=0)\n    group_id = tl.load(tile_to_group_ptr + pid)\n    tile_idx = tl.load(tile_offset_ptr + pid)\n\n    # 2. Load problem dimensions for this group\n    # M varies per group, N is loaded (typically constant in MoE but supported as variable)\n    M = tl.load(M_array + group_id)\n    N = tl.load(N_array + group_id)\n\n    # 3. Compute tile coordinates within the group\n    # Number of N tiles for this group\n    num_tiles_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m = tile_idx // num_tiles_n\n    pid_n = tile_idx % num_tiles_n\n\n    # 4. Calculate memory offsets\n    # A is concatenated. Start index depends on M_cumsum\n    m_start = tl.load(M_cumsum_ptr + group_id)\n    \n    # Base A pointer: A_ptr points to the start of the concatenated buffer\n    # We assume A is row-major packed: [Sum(M), K]. Stride M is K.\n    # offset_am = m_start + pid_m * BLOCK_M\n    \n    # Base B pointer: B is in a list of pointers. Load the specific pointer for this group.\n    # B_ptr_array is an array of pointers (int64/uint64). We cast to pointer type.\n    ptr_B_val = tl.load(B_ptr_array + group_id)\n    base_B_ptr = tl.make_block_ptr(ptr_B_val, shape=(1,1), strides=(1,1), block_shape=(1,1), order=(0,1)) if False else ptr_B_val.to(tl.pointer_type(tl.float8e4b8))\n\n    # Scales: Assume layout [scale_a, scale_b] per group interleaved\n    scale_a = tl.load(scales_ptr + group_id * 2)\n    scale_b = tl.load(scales_ptr + group_id * 2 + 1)\n\n    # 5. Iteration Loop parameters\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # A Pointers: A_base + (m_row * K) + (k_col * stride_ak)\n    # Note: K is the width of A. We assume standard row-major layout for A where stride_M = K.\n    a_ptrs = A_ptr + (m_start + offs_am[:, None]) * K + offs_k[None, :] * stride_ak\n\n    # B Pointers: B_base + (k_row * stride_bk) + (n_col * stride_bn)\n    b_ptrs = base_B_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Accumulator\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        # Load A tile with masking\n        # Mask checks M bounds and K bounds\n        mask_a = (offs_am[:, None] < M) & (k * BLOCK_K + offs_k[None, :] < K)\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n\n        # Load B tile with masking\n        # Mask checks N bounds and K bounds\n        mask_b = (offs_bn[None, :] < N) & (k * BLOCK_K + offs_k[:, None] < K)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n\n        # Explicit cast to target FP8 type for tensor cores (AMD)\n        a_fp8 = a.to(tl.float8e4b8)\n        b_fp8 = b.to(tl.float8e4b8)\n\n        # Matrix Multiplication\n        accumulator = tl.dot(a_fp8, b_fp8, accumulator)\n\n        # Advance pointers\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    # 6. Epilogue\n    # Apply scaling factors\n    result = accumulator * scale_a * scale_b\n    result = result.to(tl.bfloat16)\n\n    # Store C\n    # C is concatenated [Sum(M), N]. We assume row-major packing.\n    # Stride for M in C is N. Stride for N in C is stride_ck (usually 1).\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    \n    # C Pointers\n    c_ptrs = C_ptr + (m_start + offs_cm[:, None]) * N * stride_ck + offs_cn[None, :] * stride_ck\n    \n    mask_c = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, result, mask=mask_c)\n\nclass OptimizedGroupedGEMM:\n    def __init__(self, B_list: List[torch.Tensor], device='cuda'):\n        self.device = device\n        self.B_list = B_list\n        self.num_groups = len(B_list)\n        \n        # Pre-process B pointers\n        b_ptrs = [b.data_ptr() for b in B_list]\n        self.B_ptr_array = torch.tensor(b_ptrs, dtype=torch.int64, device=device)\n        \n        # Extract N and K from B_list (assuming B is [K, N] or [N, K] based on strides)\n        # We assume all B have same shape for simplicity in N extraction, but code handles per-group N.\n        self.N_array_host = [b.shape[1] for b in B_list] # Assuming B is [K, N]\n        self.K = B_list[0].shape[0]\n        self.N_array = torch.tensor(self.N_array_host, dtype=torch.int32, device=device)\n        \n        # Strides for B (assuming all have same layout)\n        self.stride_bk = B_list[0].stride(0)\n        self.stride_bn = B_list[0].stride(1)\n        \n        # Dummy scales for this example (1.0)\n        self.scales = torch.ones(self.num_groups * 2, dtype=torch.float32, device=device)\n\n    def _compute_schedule(self, M_splits: List[int], BLOCK_M=128, BLOCK_N=128):\n        tile_to_group = []\n        tile_offset = []\n        total_tiles = 0\n        \n        for group_id, m in enumerate(M_splits):\n            n = self.N_array_host[group_id]\n            grid_m = (m + BLOCK_M - 1) // BLOCK_M\n            grid_n = (n + BLOCK_N - 1) // BLOCK_N\n            group_tiles = grid_m * grid_n\n            \n            tile_to_group.extend([group_id] * group_tiles)\n            tile_offset.extend(range(group_tiles))\n            total_tiles += group_tiles\n            \n        return torch.tensor(tile_to_group, dtype=torch.int32, device=self.device), \\\n               torch.tensor(tile_offset, dtype=torch.int32, device=self.device), \\\n               total_tiles\n\n    def _get_schedule(self, M_splits: List[int]):\n        # Wrapper to get default block sizes and compute schedule\n        return self._compute_schedule(M_splits)\n\n    def __call__(self, A_concat: torch.Tensor, M_splits: List[int]):\n        # Prepare metadata\n        M_array = torch.tensor(M_splits, dtype=torch.int32, device=self.device)\n        M_cumsum = torch.cumsum(torch.tensor([0] + M_splits, dtype=torch.int32, device=self.device), 0)\n        \n        # Schedule\n        tile_to_group, tile_offset, total_tiles = self._get_schedule(M_splits)\n        \n        # Check output size\n        total_M = sum(M_splits)\n        # Assuming uniform N for output allocation or max N. \n        # Since C is concatenated, we calculate total size. For strictly rectangular C, N must be uniform.\n        # If N varies, C output buffer logic needs to be complex (ragged). \n        # Here we assume max N or uniform N for the output tensor allocation.\n        max_N = max(self.N_array_host)\n        C_concat = torch.empty((total_M, max_N), dtype=torch.bfloat16, device=self.device)\n        \n        BLOCK_M = 128\n        BLOCK_N = 128\n        BLOCK_K = 32 # Small K block for FP8 typically\n        GROUP_SIZE_M = 8\n        \n        grid = (total_tiles, 1, 1)\n        \n        grouped_fused_fp8_gemm_kernel_v2[grid](\n            A_ptr=A_concat,\n            C_ptr=C_concat,\n            B_ptr_array=self.B_ptr_array,\n            M_cumsum_ptr=M_cumsum,\n            M_array=M_array,\n            N_array=self.N_array,\n            K=self.K,\n            scales_ptr=self.scales,\n            tile_to_group_ptr=tile_to_group,\n            tile_offset_ptr=tile_offset,\n            stride_ak=A_concat.stride(1),\n            stride_bk=self.stride_bk,\n            stride_bn=self.stride_bn,\n            stride_ck=C_concat.stride(1),\n            total_tiles=total_tiles,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n            BLOCK_K=BLOCK_K,\n            GROUP_SIZE_M=GROUP_SIZE_M,\n            NUM_GROUPS=self.num_groups,\n            num_warps=4\n        )\n        \n        return C_concat\n\ndef test_grouped_gemm():\n    torch.manual_seed(0)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cpu\":\n        print(\"Skipping test on CPU\")\n        return\n\n    # Setup parameters\n    num_groups = 3\n    K = 128\n    N = 256\n    M_splits = [64, 128, 32]\n    total_M = sum(M_splits)\n    \n    # Generate Data\n    # A: [Total_M, K] stored as int8 (simulating fp8)\n    A = torch.randint(-10, 10, (total_M, K), dtype=torch.int8, device=device)\n    \n    # B: List of [K, N] tensors\n    B_list = [torch.randint(-10, 10, (K, N), dtype=torch.int8, device=device) for _ in range(num_groups)]\n    \n    # Run Kernel Wrapper\n    gemm_op = OptimizedGroupedGEMM(B_list, device=device)\n    C_out = gemm_op(A, M_splits)\n    \n    print(f\"Kernel executed. Output shape: {C_out.shape}\")\n    print(\"Output snippet:\", C_out[0:5, 0:5])\n\nif __name__ == \"__main__\":\n    test_grouped_gemm()", "speedup": 0.0}

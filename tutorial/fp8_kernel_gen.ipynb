{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cfadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Define the kernel with auto-tuning for AMD MI325X configurations.\n",
    "# CDNA 3 benefits from larger block sizes and moderate warp counts.\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # Config 1: Large tile, high concurrency. Good for large matrices.\n",
    "        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        # Config 2: Balanced tile. Often the sweet spot for MFMA instructions.\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n",
    "        # Config 3: Smaller tile for better occupancy on smaller batches.\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64,  'BLOCK_K': 64,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        # Config 4: High K-blocking to feed the tensor cores longer.\n",
    "        triton.Config({'BLOCK_M': 64,  'BLOCK_N': 128, 'BLOCK_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def fp8_gemm_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    # Pointers to quantization scales (scalar or vector)\n",
    "    a_scale_ptr, b_scale_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # Strides (Row-major assumed: stride_ak=1, stride_bk=1 usually for optimal load)\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes C = (scale_a * A_fp8) x (scale_b * B_fp8)\n",
    "    Input: A, B in FP8 (e4m3 or e5m2)\n",
    "    Output: C in BF16 or FP16 (accumulated in FP32)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # 1. Map Program ID to Block Coordinate\n",
    "    # -----------------------------------------------------------\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. Setup Memory Pointers\n",
    "    # -----------------------------------------------------------\n",
    "    # Create pointers for the first block of A and B\n",
    "    # A is usually [M, K], B is usually [K, N] (transposed in memory usually helps, strictly K-major)\n",
    "    \n",
    "    # Range for A rows [0..BLOCK_M]\n",
    "    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n",
    "    # Range for B cols [0..BLOCK_N]\n",
    "    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n",
    "    # Range for K dimension [0..BLOCK_K]\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "\n",
    "    # Pointer arithmetic\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3. Load Scales (Quantization)\n",
    "    # -----------------------------------------------------------\n",
    "    # Assuming per-tensor quantization for simplicity (scalar load). \n",
    "    # If per-row/col, you would load vectors based on offs_am / offs_bn.\n",
    "    scale_a = tl.load(a_scale_ptr)\n",
    "    scale_b = tl.load(b_scale_ptr)\n",
    "    \n",
    "    # Accumulator (MFMA accumulates in F32)\n",
    "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. Main Loop (K-Dimension)\n",
    "    # -----------------------------------------------------------\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_K)):\n",
    "        # Load the next blocks of A and B\n",
    "        # Masking is required if K is not a multiple of BLOCK_K, \n",
    "        # but for performance we usually pad K to be a multiple of BLOCK_K.\n",
    "        # Here we assume padded K for raw speed (no 'mask=' argument).\n",
    "        \n",
    "        a = tl.load(a_ptrs)\n",
    "        b = tl.load(b_ptrs)\n",
    "        \n",
    "        # We perform the Dot Product. \n",
    "        # Triton detects that inputs are FP8 (e.g. float8e4nv) and \n",
    "        # maps this to the CDNA 3 `mfma_f32_32x32x16_fp8_fp8` instruction.\n",
    "        accumulator = tl.dot(a, b, accumulator)\n",
    "\n",
    "        # Advance pointers to the next K-block\n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5. Epilogue: Scaling & Store\n",
    "    # -----------------------------------------------------------\n",
    "    # Convert accumulator to result type (e.g., float16 or bfloat16)\n",
    "    c = accumulator.to(tl.float32)\n",
    "    \n",
    "    # Apply Quantization Scales (Dequantize)\n",
    "    # C = (A_scaled * B_scaled) = (A_raw * s_a) * (B_raw * s_b) = Acc * s_a * s_b\n",
    "    total_scale = scale_a * scale_b\n",
    "    c = c * total_scale\n",
    "\n",
    "    # Store result\n",
    "    # We use a mask for M/N in case matrix dims aren't perfect multiples of BLOCK sizes\n",
    "    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    \n",
    "    # Cast to output format (BF16 is standard for AI on MI300/325)\n",
    "    tl.store(c_ptrs, c.to(tl.bfloat16), mask=c_mask)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Wrapper Function\n",
    "# -------------------------------------------------------------------------\n",
    "def triton_fp8_matmul(a_fp8, b_fp8, scale_a, scale_b):\n",
    "    \"\"\"\n",
    "    a_fp8: (M, K)\n",
    "    b_fp8: (K, N)  <- Note: In memory, usually stored (N, K) for efficient loads, \n",
    "                      but for simplicity here we assume standard (K,N) or handle strides.\n",
    "    \"\"\"\n",
    "    M, K = a_fp8.shape\n",
    "    K_b, N = b_fp8.shape\n",
    "    assert K == K_b, \"Matrix dimensions mismatch\"\n",
    "\n",
    "    # Allocate output\n",
    "    c = torch.empty((M, N), device=a_fp8.device, dtype=torch.bfloat16)\n",
    "    \n",
    "    # Grid definition\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), )\n",
    "\n",
    "    # Launch kernel\n",
    "    # NOTE: We assume 'b_fp8' is Column-Major (standard PyTorch layout is Row-Major).\n",
    "    # If B is (K, N) Row-Major: stride_bk=N, stride_bn=1\n",
    "    # For optimal performance, B should be (N, K) in memory (Transposed), \n",
    "    # so reading a column of B is contiguous.\n",
    "    # Here we simply pass the strides PyTorch gives us.\n",
    "    fp8_gemm_kernel[grid](\n",
    "        a_fp8, b_fp8, c,\n",
    "        scale_a, scale_b,\n",
    "        M, N, K,\n",
    "        a_fp8.stride(0), a_fp8.stride(1),\n",
    "        b_fp8.stride(0), b_fp8.stride(1),\n",
    "        c.stride(0), c.stride(1)\n",
    "    )\n",
    "    return c\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Unit Test: Numerical Verification\n",
    "# -------------------------------------------------------------------------\n",
    "def test_correctness():\n",
    "    torch.manual_seed(0)\n",
    "    # Dimensions (Multiples of 128 are best for Triton)\n",
    "    M, N, K = 1024, 1024, 1024\n",
    "    \n",
    "    # Use AMD-specific FP8 type if available, else standard e4m3\n",
    "    fp8_dtype = torch.float8_e4m3fnuz if hasattr(torch, 'float8_e4m3fnuz') else torch.float8_e4m3fn\n",
    "    print(f\"Testing with dtype: {fp8_dtype}\")\n",
    "\n",
    "    # 1. Prepare Inputs (BF16)\n",
    "    a_bf16 = torch.randn((M, K), device='cuda', dtype=torch.bfloat16)\n",
    "    b_bf16 = torch.randn((K, N), device='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "    # 2. Mock Quantization (Per-Tensor)\n",
    "    # Calculate simple max-based scale\n",
    "    scale_a_val = a_bf16.abs().max().item() / 240.0 # 240 is approx max range for e4m3\n",
    "    scale_b_val = b_bf16.abs().max().item() / 240.0\n",
    "    \n",
    "    scale_a = torch.tensor(scale_a_val, device='cuda', dtype=torch.float32)\n",
    "    scale_b = torch.tensor(scale_b_val, device='cuda', dtype=torch.float32)\n",
    "\n",
    "    # Quantize to FP8\n",
    "    a_fp8 = (a_bf16 / scale_a).to(fp8_dtype)\n",
    "    b_fp8 = (b_bf16 / scale_b).to(fp8_dtype)\n",
    "\n",
    "    # 3. Reference Implementation (Torch High Precision)\n",
    "    # We \"dequantize\" the FP8 inputs back to BF16 to see exactly what the GPU sees,\n",
    "    # then matmul in BF16. This isolates kernel logic errors from quantization noise.\n",
    "    a_ref = a_fp8.to(torch.bfloat16)\n",
    "    b_ref = b_fp8.to(torch.bfloat16)\n",
    "    c_ref = torch.matmul(a_ref, b_ref) * (scale_a * scale_b)\n",
    "\n",
    "    # 4. Triton Implementation\n",
    "    c_tri = triton_fp8_matmul(a_fp8, b_fp8, scale_a, scale_b)\n",
    "\n",
    "    # 5. Compare\n",
    "    # Tolerances: FP8 has low precision. We expect some deviation.\n",
    "    # Comparing against the 'simulated' reference (a_ref @ b_ref) handles quantization noise.\n",
    "    # If the kernel is correct, it should match closely.\n",
    "    print(f\"Ref Stats  - Max: {c_ref.max().item():.4f}, Min: {c_ref.min().item():.4f}, Mean: {c_ref.abs().mean().item():.4f}\")\n",
    "    print(f\"Tri Stats  - Max: {c_tri.max().item():.4f}, Min: {c_tri.min().item():.4f}, Mean: {c_tri.abs().mean().item():.4f}\")\n",
    "\n",
    "    # Check if Triton output is all zeros\n",
    "    if c_tri.abs().max().item() == 0:\n",
    "        print(\"⚠️ WARNING: Triton output is all zeros! Kernel might not be writing.\")\n",
    "\n",
    "    diff = (c_ref - c_tri).abs().max()\n",
    "    print(f\"Max Difference: {diff.item()}\")\n",
    "    \n",
    "    # Note: FP8 matmul accumulation order on hardware might differ from PyTorch BF16 matmul,\n",
    "    # so we use a loose tolerance (atol=1e-1 or similar depending on magnitude).\n",
    "    torch.testing.assert_close(c_tri, c_ref, atol=0.0002, rtol=0.01)\n",
    "    print(\"✅ Unit Test Passed!\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Benchmark Script\n",
    "# -------------------------------------------------------------------------\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['M', 'N', 'K'], \n",
    "        x_vals=[1024, 2048, 4096, 8192], \n",
    "        line_arg='provider', \n",
    "        line_vals=['torch-bf16', 'triton-fp8'], \n",
    "        line_names=['PyTorch (BF16)', 'Triton (FP8)'],\n",
    "        styles=[('green', '-'), ('blue', '-')], \n",
    "        ylabel='TFLOPS', \n",
    "        plot_name='fp8-gemm-performance',\n",
    "        args={}\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, K, provider):\n",
    "    # Setup\n",
    "    fp8_dtype = torch.float8_e4m3fnuz if hasattr(torch, 'float8_e4m3fnuz') else torch.float8_e4m3fn\n",
    "    \n",
    "    a_fp8 = torch.randn((M, K), device='cuda', dtype=torch.float16).to(fp8_dtype)\n",
    "    b_fp8 = torch.randn((K, N), device='cuda', dtype=torch.float16).to(fp8_dtype)\n",
    "    scale_a = torch.tensor(1.0, device='cuda', dtype=torch.float32)\n",
    "    scale_b = torch.tensor(1.0, device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    # For PyTorch baseline, we use BF16 (Standard \"High Perf\" baseline)\n",
    "    a_bf16 = torch.randn((M, K), device='cuda', dtype=torch.bfloat16)\n",
    "    b_bf16 = torch.randn((K, N), device='cuda', dtype=torch.bfloat16)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'torch-bf16':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a_bf16, b_bf16), quantiles=quantiles)\n",
    "    if provider == 'triton-fp8':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_fp8_matmul(a_fp8, b_fp8, scale_a, scale_b), quantiles=quantiles)\n",
    "        \n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919e2a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with dtype: torch.float8_e4m3fnuz\n",
      "Ref Stats  - Max: 165.0000, Min: -157.0000, Mean: 25.6250\n",
      "Tri Stats  - Max: 165.0000, Min: -156.0000, Mean: 25.6250\n",
      "Max Difference: 1.0\n",
      "✅ Unit Test Passed!\n"
     ]
    }
   ],
   "source": [
    "test_correctness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5351f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp8-gemm-performance:\n",
      "        M       N       K  PyTorch (BF16)  Triton (FP8)\n",
      "0  1024.0  1024.0  1024.0      105.268802    111.621381\n",
      "1  2048.0  2048.0  2048.0      468.371547    456.425835\n",
      "2  4096.0  4096.0  4096.0      788.261761    622.580294\n",
      "3  8192.0  8192.0  8192.0      767.194707    651.595634\n"
     ]
    }
   ],
   "source": [
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1184bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "\n",
    "# Ensure you are on a ROCm system with MI300/MI325\n",
    "# FP8 data types in PyTorch: torch.float8_e4m3fnuz (AMD standard) or e5m2\n",
    "\n",
    "def run_fp8_gemm(M, N, K):\n",
    "    # 1. Prepare Data\n",
    "    # AMD MI300/325 often uses e4m3fnuz for weights/activations in inference\n",
    "    dtype_input = torch.float8_e4m3fnuz \n",
    "    dtype_output = torch.bfloat16\n",
    "\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.float16).to(dtype_input)\n",
    "    b = torch.randn((N, K), device='cuda', dtype=torch.float16).t().to(dtype_input) # B is KxN, stored as N,K (Col Major) or K,N?\n",
    "    # NOTE: Optimal layout for GEMM is typically A=RowMajor, B=ColMajor (K is the inner dimension for both)\n",
    "    # Triton loads row-major by default. If B is KxN, we want stride_bk=1. \n",
    "    # If B is NxK (transposed), we want stride_bn=1.\n",
    "    \n",
    "    # For this kernel code:\n",
    "    # A is (M, K) Row Major -> stride_am=K, stride_ak=1\n",
    "    # B is (K, N) Col Major -> stride_bk=1, stride_bn=K\n",
    "    \n",
    "    # Let's verify B layout. If we pass B as (K, N) contig:\n",
    "    # b_ptr + k * stride_bk + n * stride_bn\n",
    "    b = torch.randn((K, N), device='cuda', dtype=torch.float16).to(dtype_input)\n",
    "\n",
    "    c = torch.empty((M, N), device='cuda', dtype=dtype_output)\n",
    "    \n",
    "    scale_a = torch.tensor(0.5, device='cuda', dtype=torch.float32)\n",
    "    scale_b = torch.tensor(0.5, device='cuda', dtype=torch.float32)\n",
    "\n",
    "    # 2. Enqueue Kernel\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), )\n",
    "    \n",
    "    fp8_gemm_kernel[grid](\n",
    "        a, b, c,\n",
    "        scale_a, scale_b,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1)\n",
    "    )\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ef145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18.7500,  -5.8750, -10.8750,  ..., -24.7500, -15.8750, -15.7500],\n",
       "        [ 11.0625,  -6.5000,  -7.1250,  ...,  -0.6055,   5.5000, -15.9375],\n",
       "        [ 24.8750, -34.5000,  20.8750,  ...,  -0.0815,   7.4688,  10.8125],\n",
       "        ...,\n",
       "        [-26.1250,  22.6250,  15.3750,  ...,  -9.5625, -11.7500,   8.8125],\n",
       "        [ 18.5000, -16.3750,  11.5625,  ...,  -7.7188,  23.6250,  -1.5234],\n",
       "        [ -2.9062,   6.0312, -25.1250,  ...,  14.0625,  22.8750,   9.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example run\n",
    "M, N, K = 4096, 4096, 4096\n",
    "output = run_fp8_gemm(M, N, K)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dc6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43415513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86ec914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Comparative Unit Test: Triton vs TE vs PyTorch(BF16)...\n",
      "Configurations: [128, 256, 512] (Rows)\n",
      "Running Triton Fused Kernel...\n",
      "Running Transformer Engine...\n",
      "\n",
      "============================================================\n",
      "ID   Size             Diff(Tri, Ref)   Diff(TE, Ref)    Diff(Tri, TE)   \n",
      "============================================================\n",
      "0    128x128x128      2.0000           2.0000           0.0000          \n",
      "1    256x128x128      2.0000           2.0000           0.0625          \n",
      "2    512x128x128      2.0000           2.0000           0.0000          \n",
      "============================================================\n",
      "Note: Single digit diffs (e.g. 1.0 - 5.0) are expected for FP8 vs BF16.\n",
      "      Diff(Tri, TE) should ideally be small, but scaling impl details may vary.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import transformer_engine.pytorch as te\n",
    "from transformer_engine.pytorch.module.grouped_linear import GroupedLinear\n",
    "from transformer_engine.common.recipe import Float8CurrentScaling, Format\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Fused Grouped FP8 GEMM Kernel (Quantize + GEMM)\n",
    "# -------------------------------------------------------------------------\n",
    "# Feature Parity with Transformer Engine Current Scaling:\n",
    "# 1. Inputs are BF16/FP16 (High Precision).\n",
    "# 2. Kernel fuses Quantization (Scaling + Cast) with GEMM.\n",
    "# 3. Scales are provided (calculated per-tensor on host/wrapper).\n",
    "@triton.jit\n",
    "def grouped_fused_fp8_gemm_kernel(\n",
    "    # Arrays of Pointers (One per GEMM)\n",
    "    a_ptr_list, b_ptr_list, c_ptr_list,\n",
    "    # Scales for Quantization (BF16 -> FP8)\n",
    "    a_qscale_ptr_list, b_qscale_ptr_list,\n",
    "    # Scales for De-quantization (Accumulator -> BF16)\n",
    "    a_dscale_ptr_list, b_dscale_ptr_list,\n",
    "    # Arrays of Dimensions\n",
    "    M_list, N_list, K_list,\n",
    "    # Grid Scheduling Map\n",
    "    grid_map_ptr,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr, \n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # --- 1. Scheduler / Swizzling ---\n",
    "    map_offset = pid * 3\n",
    "    gemm_id = tl.load(grid_map_ptr + map_offset)\n",
    "    pid_m = tl.load(grid_map_ptr + map_offset + 1)\n",
    "    pid_n = tl.load(grid_map_ptr + map_offset + 2)\n",
    "\n",
    "    # --- 2. Load Metadata ---\n",
    "    M = tl.load(M_list + gemm_id)\n",
    "    N = tl.load(N_list + gemm_id)\n",
    "    K = tl.load(K_list + gemm_id)\n",
    "    \n",
    "    # Inputs are now BF16 (High Precision)\n",
    "    a_ptr = tl.load(a_ptr_list + gemm_id).to(tl.pointer_type(tl.bfloat16))\n",
    "    b_ptr = tl.load(b_ptr_list + gemm_id).to(tl.pointer_type(tl.bfloat16))\n",
    "    c_ptr = tl.load(c_ptr_list + gemm_id).to(tl.pointer_type(tl.bfloat16))\n",
    "    \n",
    "    # Scales\n",
    "    qscale_a = tl.load(tl.load(a_qscale_ptr_list + gemm_id).to(tl.pointer_type(tl.float32)))\n",
    "    qscale_b = tl.load(tl.load(b_qscale_ptr_list + gemm_id).to(tl.pointer_type(tl.float32)))\n",
    "    dscale_a = tl.load(tl.load(a_dscale_ptr_list + gemm_id).to(tl.pointer_type(tl.float32)))\n",
    "    dscale_b = tl.load(tl.load(b_dscale_ptr_list + gemm_id).to(tl.pointer_type(tl.float32)))\n",
    "\n",
    "    # --- 3. Pointer Arithmetic ---\n",
    "    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "   \n",
    "    # --- 4. Main Loop (Fused Quantization + MMA) ---\n",
    "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_K)):\n",
    "        a_bf16 = tl.load(a_ptrs)\n",
    "        b_bf16 = tl.load(b_ptrs)\n",
    "        \n",
    "        # Fused Quantization: Scale + Cast to FP8\n",
    "        # Using float8e4b8 (AMD E4M3FNUZ equivalent)\n",
    "        a_fp8 = (a_bf16 * qscale_a).to(tl.float8e4b8)\n",
    "        b_fp8 = (b_bf16 * qscale_b).to(tl.float8e4b8)\n",
    "        \n",
    "        accumulator = tl.dot(a_fp8, b_fp8, accumulator)\n",
    "        \n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "\n",
    "    # --- 5. Epilogue (Dequantization) ---\n",
    "    total_dscale = dscale_a * dscale_b\n",
    "    c = accumulator.to(tl.float32) * total_dscale\n",
    "    \n",
    "    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    \n",
    "    tl.store(c_ptrs, c.to(tl.bfloat16), mask=c_mask)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Python Wrapper: Triton Fused\n",
    "# -------------------------------------------------------------------------\n",
    "def run_fused_grouped_gemm(A_list, B_list):\n",
    "    device = A_list[0].device\n",
    "    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 128\n",
    "    GROUP_SIZE_M = 8 \n",
    "    FP8_MAX = 240.0\n",
    "    \n",
    "    grid_map = [] \n",
    "    M_list, N_list, K_list = [], [], []\n",
    "    A_ptr_list, B_ptr_list, C_ptr_list = [], [], []\n",
    "    qscale_a_list, qscale_b_list = [], []\n",
    "    dscale_a_list, dscale_b_list = [], []\n",
    "    C_out_list = []\n",
    "    holder_scales = [] \n",
    "\n",
    "    for idx, (A, B) in enumerate(zip(A_list, B_list)):\n",
    "        M, K = A.shape\n",
    "        _, N = B.shape\n",
    "        C = torch.empty((M, N), device=device, dtype=torch.bfloat16)\n",
    "        C_out_list.append(C)\n",
    "        \n",
    "        # Dynamic Current Scaling\n",
    "        max_a = A.abs().max().float()\n",
    "        max_b = B.abs().max().float()\n",
    "        max_a = torch.maximum(max_a, torch.tensor(1e-6, device=device))\n",
    "        max_b = torch.maximum(max_b, torch.tensor(1e-6, device=device))\n",
    "        \n",
    "        q_scale_a = (FP8_MAX / max_a)\n",
    "        q_scale_b = (FP8_MAX / max_b)\n",
    "        d_scale_a = 1.0 / q_scale_a\n",
    "        d_scale_b = 1.0 / q_scale_b\n",
    "        \n",
    "        # Metadata\n",
    "        M_list.append(M); N_list.append(N); K_list.append(K)\n",
    "        A_ptr_list.append(A.data_ptr()); B_ptr_list.append(B.data_ptr()); C_ptr_list.append(C.data_ptr())\n",
    "        \n",
    "        qa_t = q_scale_a.clone(); qb_t = q_scale_b.clone()\n",
    "        da_t = d_scale_a.clone(); db_t = d_scale_b.clone()\n",
    "        holder_scales.extend([qa_t, qb_t, da_t, db_t])\n",
    "        \n",
    "        qscale_a_list.append(qa_t.data_ptr())\n",
    "        qscale_b_list.append(qb_t.data_ptr())\n",
    "        dscale_a_list.append(da_t.data_ptr())\n",
    "        dscale_b_list.append(db_t.data_ptr())\n",
    "\n",
    "        # Scheduling\n",
    "        num_pid_m = triton.cdiv(M, BLOCK_M)\n",
    "        num_pid_n = triton.cdiv(N, BLOCK_N)\n",
    "        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "        total_tiles = num_pid_m * num_pid_n\n",
    "        \n",
    "        for pid in range(total_tiles):\n",
    "            group_id = pid // num_pid_in_group\n",
    "            first_pid_m = group_id * GROUP_SIZE_M\n",
    "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "            pid_m = first_pid_m + (pid % group_size_m)\n",
    "            pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "            grid_map.append([idx, pid_m, pid_n])\n",
    "\n",
    "    d_grid_map = torch.tensor(grid_map, device=device, dtype=torch.int32)\n",
    "    d_M = torch.tensor(M_list, device=device, dtype=torch.int32)\n",
    "    d_N = torch.tensor(N_list, device=device, dtype=torch.int32)\n",
    "    d_K = torch.tensor(K_list, device=device, dtype=torch.int32)\n",
    "    d_Ap = torch.tensor(A_ptr_list, device=device, dtype=torch.int64)\n",
    "    d_Bp = torch.tensor(B_ptr_list, device=device, dtype=torch.int64)\n",
    "    d_Cp = torch.tensor(C_ptr_list, device=device, dtype=torch.int64)\n",
    "    d_QSa = torch.tensor(qscale_a_list, device=device, dtype=torch.int64)\n",
    "    d_QSb = torch.tensor(qscale_b_list, device=device, dtype=torch.int64)\n",
    "    d_DSa = torch.tensor(dscale_a_list, device=device, dtype=torch.int64)\n",
    "    d_DSb = torch.tensor(dscale_b_list, device=device, dtype=torch.int64)\n",
    "    \n",
    "    grid = (len(grid_map), 1, 1)\n",
    "    sam, sak = A_list[0].stride(); sbk, sbn = B_list[0].stride(); scm, scn = C_out_list[0].stride()\n",
    "    \n",
    "    grouped_fused_fp8_gemm_kernel[grid](\n",
    "        d_Ap, d_Bp, d_Cp, \n",
    "        d_QSa, d_QSb, d_DSa, d_DSb,\n",
    "        d_M, d_N, d_K, d_grid_map,\n",
    "        sam, sak, sbk, sbn, scm, scn,\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, GROUP_SIZE_M=GROUP_SIZE_M,\n",
    "        num_warps=4, num_stages=3\n",
    "    )\n",
    "    return C_out_list\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Python Wrapper: Transformer Engine\n",
    "# -------------------------------------------------------------------------\n",
    "def run_te_grouped_gemm(A_list, B_list):\n",
    "    \"\"\"\n",
    "    Runs Grouped GEMM using Transformer Engine.\n",
    "    TE performs: Out = In * Weight^T roughly.\n",
    "    Inputs: List of BF16 Tensors A(M, K) and B(K, N)\n",
    "    \"\"\"\n",
    "    device = A_list[0].device\n",
    "    groups = len(A_list)\n",
    "    M_splits = [a.shape[0] for a in A_list]\n",
    "    K = A_list[0].shape[1]\n",
    "    N = B_list[0].shape[1]\n",
    "    \n",
    "    # 1. Concatenate Inputs into one huge tensor (Total_M, K)\n",
    "    X = torch.cat(A_list, dim=0).contiguous()\n",
    "    \n",
    "    # 2. Prepare Weights\n",
    "    # TE.GroupedLinear expects separate parameters for each GEMM\n",
    "    \n",
    "    # 3. Create Module\n",
    "    mod = GroupedLinear(\n",
    "        num_gemms=groups,\n",
    "        in_features=K,\n",
    "        out_features=N,\n",
    "        bias=False,\n",
    "        device=device,\n",
    "        params_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # 4. Inject Weights (TE stores weights individually as weight0, weight1...)\n",
    "    with torch.no_grad():\n",
    "        for i, b in enumerate(B_list):\n",
    "            # B is (K, N), TE Weight is (N, K)\n",
    "            w = b.t().contiguous()\n",
    "            # Access dynamic attribute weight{i}\n",
    "            getattr(mod, f\"weight{i}\").copy_(w)\n",
    "        \n",
    "    # 5. Execute with FP8 Autocast\n",
    "    fp8_recipe = Float8CurrentScaling(fp8_format=Format.E4M3)\n",
    "    \n",
    "    with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "        Y = mod(X, M_splits)\n",
    "        \n",
    "    # 6. Split Output back to list\n",
    "    C_list = torch.split(Y, M_splits)\n",
    "    return list(C_list)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Unit Test: Comparative Accuracy\n",
    "# -------------------------------------------------------------------------\n",
    "def test_grouped_gemm_correctness():\n",
    "    print(\"Running Comparative Unit Test: Triton vs TE vs PyTorch(BF16)...\")\n",
    "    torch.manual_seed(0)\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Test Config\n",
    "    fixed_N, fixed_K = 128, 128\n",
    "    # Using aligned sizes for TE safety\n",
    "    problem_sizes = [(128, fixed_N, fixed_K), (256, fixed_N, fixed_K), (512, fixed_N, fixed_K)]\n",
    "    \n",
    "    A_list_bf16, B_list_bf16 = [], []\n",
    "    Ref_C_list = []\n",
    "\n",
    "    print(f\"Configurations: {[p[0] for p in problem_sizes]} (Rows)\")\n",
    "\n",
    "    for M, N, K in problem_sizes:\n",
    "        a = torch.randn((M, K), device=device, dtype=torch.bfloat16)\n",
    "        b = torch.randn((K, N), device=device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # 1. Reference: Pure BF16 Math\n",
    "        c_ref = torch.matmul(a, b)\n",
    "        \n",
    "        A_list_bf16.append(a)\n",
    "        B_list_bf16.append(b)\n",
    "        Ref_C_list.append(c_ref)\n",
    "\n",
    "    # 2. Run Triton Fused\n",
    "    print(\"Running Triton Fused Kernel...\")\n",
    "    Triton_C_list = run_fused_grouped_gemm(A_list_bf16, B_list_bf16)\n",
    "    \n",
    "    # 3. Run Transformer Engine\n",
    "    print(\"Running Transformer Engine...\")\n",
    "    TE_C_list = run_te_grouped_gemm(A_list_bf16, B_list_bf16)\n",
    "\n",
    "    # 4. Compare\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{'ID':<4} {'Size':<16} {'Diff(Tri, Ref)':<16} {'Diff(TE, Ref)':<16} {'Diff(Tri, TE)':<16}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, (M, N, K) in enumerate(problem_sizes):\n",
    "        c_ref = Ref_C_list[i]\n",
    "        c_tri = Triton_C_list[i]\n",
    "        c_te  = TE_C_list[i]\n",
    "        \n",
    "        diff_tri_ref = (c_tri - c_ref).abs().max().item()\n",
    "        diff_te_ref  = (c_te - c_ref).abs().max().item()\n",
    "        diff_tri_te  = (c_tri - c_te).abs().max().item()\n",
    "        \n",
    "        print(f\"{i:<4} {f'{M}x{N}x{K}':<16} {diff_tri_ref:<16.4f} {diff_te_ref:<16.4f} {diff_tri_te:<16.4f}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Note: Single digit diffs (e.g. 1.0 - 5.0) are expected for FP8 vs BF16.\")\n",
    "    print(\"      Diff(Tri, TE) should ideally be small, but scaling impl details may vary.\")\n",
    "\n",
    "test_grouped_gemm_correctness()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

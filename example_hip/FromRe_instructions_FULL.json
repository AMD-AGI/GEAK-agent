[
  {
    "task": "bitonic_sort",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "rocm-examples/Applications/bitonic_sort",
    "file_name": "main.hip"
  },
  {
    "task": "convolution",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "rocm-examples/Applications/convolution",
    "file_name": "main.hip"
  },
  {
    "task": "floyd_warshall",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "rocm-examples/Applications/floyd_warshall",
    "file_name": "main.hip"
  },
  {
    "task": "histogram",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "rocm-examples/Applications/histogram",
    "file_name": "main.hip"

  },
  {
    "task": "monte_carlo_pi",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "rocm-examples/Applications/monte_carlo_pi",
    "file_name": "main.hip"

  },
  {
    "task": "prefix_sum",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "rocm-examples/Applications/prefix_sum",
    "file_name": "main.hip"
  },
  {
    "task": "point_to_voxelidx",
    "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
    "output": "",
    "file_path": "point_to_voxel",
    "file_name": "main.hip"
  },
  {
      "task": "assign_score_withk",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/assign_score_withk",
      "file_name": "src/assign_score_withk_cuda.hip"
  },
  {
      "task": "ball_query",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/ball_query",
      "file_name": "src/ball_query_cuda.hip"
  },
  {
      "task": "furthest_point_sample",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/furthest_point_sample",
      "file_name": "src/furthest_point_sample_cuda.hip"
  },
  {
      "task": "gather_points",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/gather_points",
      "file_name": "src/gather_points_cuda.hip"
  },
  {
      "task": "knn",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/knn",
      "file_name": "src/knn_cuda.hip"
  },
  {
      "task": "points_in_boxes",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/points_in_boxes",
      "file_name": "src/points_in_boxes_cuda.hip"
  },
  {
      "task": "roiaware_pool3d",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/roiaware_pool3d",
      "file_name": "src/roiaware_pool3d_kernel.hip"
  },
  {
      "task": "roipoint_pool3d",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/roipoint_pool3d",
      "file_name": "src/roipoint_pool3d_kernel.hip"
  },
  {
      "task": "three_interpolate",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/three_interpolate",
      "file_name": "src/three_interpolate_cuda.hip"
  },
  {
      "task": "three_nn",
      "instruction": "Please optimize the following HIP implementation for better performance. Please optimize this implementation considering the following aspects:\n1. Memory access patterns and coalescing\n2. Branch divergence minimization\n3. Instruction-level parallelism\n4. Register usage optimization\n5. Potential use of shared memory if beneficial\n6. Warp-level optimizations\n7. Any other performance-enhancing techniques specific to AMD GPUs\n8. Optimal thread block and grid dimensions\n9. Potential kernel fusion or splitting opportunities\n10. Stream and asynchronous execution optimizations\n\nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function. ",
      "output": "",
      "file_path": "mmcv/three_nn",
      "file_name": "src/three_nn_cuda.hip"
  },
  {
    "task": "rms",
    "instruction": "Please optimize the a HIP code implementation (aimed for ROCM platform, MI308 GPU) for better performance. MI308 specs: 64KB LDS per Compute Unit (CU), 80 CUs total. Follows are some guidelines for optimization: 1. Chunked processing: Divide large data into fixed-size chunks (e.g., threads x items/elements) to fit in registers/shared memory, enable streaming computation, and minimize global memory accesses. Process each chunk independently while carrying over state. \n2. Shared memory for state propagation: Use shared memory as a buffer to handle inter-chunk dependencies, avoiding redundant global memory reads. Store and shift data for efficient access by threads. \n3. Delayed operations: Postpone writes to shared memory until after dependent reads to prevent data races and overwrites, ensuring correct sequential dependencies. \n4. Vectorized I/O: Perform loads/stores in vector types (e.g., 4 or 8 elements for float/half) for coalesced memory access. Use direct mode for aligned data or warp-transpose for flexibility, reducing instruction count and boosting bandwidth. \n5. CUB primitives: Employ CUB library for parallel operations: BlockLoad/BlockStore for efficient, coalesced input/output with temporary shared memory; BlockScan for prefix computations where needed. \n6. Loop unrolling: Apply #pragma unroll to inner loops (e.g., over dimensions or elements) to reduce branching overhead and enable compiler optimizations like instruction scheduling. \n7. Bounded accesses: Implement conditional checks in loads/stores (e.g., if index < length) to safely handle variable data sizes and prevent out-of-bounds errors. \n8. Type and feature handling: Use templates for data types (e.g., float/half/bf16, optional complex); boolean switches for optional features like activations. \n9. Resource limiting for occupancy: Reduce shared memory (LDS) and register usage per workgroup to boost occupancy, allowing more concurrent workgroups per CU/SM for improved parallelism and latency hiding. \n10. Branch divergence minimization: Structure code to minimize divergent branches within warps, ensuring threads execute the same path where possible. \n11. Instruction-level parallelism: Maximize ILP by interleaving independent instructions to hide latencies. \n12. Performance-enhancing techniques specific to AMD GPUs: Apply AMD-specific optimizations like wavefront management or ROCm-tuned configurations. \n13. Kernel fusion or splitting opportunities: Fuse multiple kernels to reduce launches and global memory traffic, or split for better resource utilization. \n 14. Stream and asynchronous execution: Use ROCm streams for overlapping computation and data transfer asynchronously. \nYou can apply other aspects of optimization that fit the kernel. \nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function.",
    "output": "",
    "file_path": "rms",
    "file_name": "rms.cu"
  },
  {
    "task": "silu",
    "instruction": "Please optimize the a HIP code implementation (aimed for ROCM platform, MI308 GPU) for better performance. MI308 specs: 64KB LDS per Compute Unit (CU), 80 CUs total. Follows are some guidelines for optimization: 1. Chunked processing: Divide large data into fixed-size chunks (e.g., threads x items/elements) to fit in registers/shared memory, enable streaming computation, and minimize global memory accesses. Process each chunk independently while carrying over state. \n2. Shared memory for state propagation: Use shared memory as a buffer to handle inter-chunk dependencies, avoiding redundant global memory reads. Store and shift data for efficient access by threads. \n3. Delayed operations: Postpone writes to shared memory until after dependent reads to prevent data races and overwrites, ensuring correct sequential dependencies. \n4. Vectorized I/O: Perform loads/stores in vector types (e.g., 4 or 8 elements for float/half) for coalesced memory access. Use direct mode for aligned data or warp-transpose for flexibility, reducing instruction count and boosting bandwidth. \n5. CUB primitives: Employ CUB library for parallel operations: BlockLoad/BlockStore for efficient, coalesced input/output with temporary shared memory; BlockScan for prefix computations where needed. \n6. Loop unrolling: Apply #pragma unroll to inner loops (e.g., over dimensions or elements) to reduce branching overhead and enable compiler optimizations like instruction scheduling. \n7. Bounded accesses: Implement conditional checks in loads/stores (e.g., if index < length) to safely handle variable data sizes and prevent out-of-bounds errors. \n8. Type and feature handling: Use templates for data types (e.g., float/half/bf16, optional complex); boolean switches for optional features like activations. \n9. Resource limiting for occupancy: Reduce shared memory (LDS) and register usage per workgroup to boost occupancy, allowing more concurrent workgroups per CU/SM for improved parallelism and latency hiding. \n10. Branch divergence minimization: Structure code to minimize divergent branches within warps, ensuring threads execute the same path where possible. \n11. Instruction-level parallelism: Maximize ILP by interleaving independent instructions to hide latencies. \n12. Performance-enhancing techniques specific to AMD GPUs: Apply AMD-specific optimizations like wavefront management or ROCm-tuned configurations. \n13. Kernel fusion or splitting opportunities: Fuse multiple kernels to reduce launches and global memory traffic, or split for better resource utilization. \n 14. Stream and asynchronous execution: Use ROCm streams for overlapping computation and data transfer asynchronously. \nYou can apply other aspects of optimization that fit the kernel. \nImportant requirements:\n1. MUST keep the exact same kernel function name \n2. MUST maintain the same kernel function signature and parameter types\n3. MUST keep the same kernel launch configuration structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST preserve the same algorithm logic and correctness\n6. MUST maintain the same comments and code formatting style\n7. If the parameter of the kernel is not used, you should remove it and not return it in the code\n8. MUST define shared_memory_size before kernel launch if using shared memory\n\nReturn the optimized implementation including:\n1. The optimized kernel function with the exact same name and signature\n2. Any modified kernel launch parameters (if needed)\n3. Any additional helper functions or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe code must be directly compilable and runnable with the same interface as the original implementation. Do not modify the input types and values used when calling the kernel in the main function.",
    "output": "",
    "file_path": "silu",
    "file_name": "silu.hip"
  },
  {
      "task": "gemm_multiply_multiply_xdl_fp8_ab_scale",
      "instruction": "Here is a GEMM benchmarking script that supports multiple device kernel instances (DeviceOpInstance0 to DeviceOpInstance18) via std::variant<DeviceOpVariant>. Each instance corresponds to a kernel with different tuning parameters (e.g., block size, M/N tile size, pipeline strategy). Design a better heuristic-based selector that picks the most suitable DeviceOpVariant based on the input GEMM shape (M, N, K). The current implementation hardcodes a single variant, which doesn't reflect optimal performance across different shapes. Modify in-place to the benchmarking script, do not change the code that is not relavent to the selection of DeviceOpVariant. Don't skip any code. MUST ensure the code is directly compilable and runnable. MUST preserve the same algorithm logic and correctness. MUST maintain the same comments and code formatting style. Consider the meaning of the arguments such as blocksize and tilesize.",
      "output": "",
      "file_path": "composable_kernel/example/65_gemm_multiply_multiply",
      "file_name": "gemm_multiply_multiply_xdl_fp8_ab_scale.cpp"
  }
]
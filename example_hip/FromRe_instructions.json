[
  {
    "task": "silu",
    "instruction": "Optimize SiLU activation kernel. Current version uses only 32 threads per block which is very inefficient. Key optimizations: increase block size to 256-1024 threads, use vectorized bf16 loads (bf16x2 or bf16x4), use __expf for faster exp, use __forceinline__. MUST keep kernel name: silu_mul_kernel and signature: (bf16* out, const bf16* in, int64_t B, int64_t H).",
    "file_path": "silu",
    "file_name": "silu.hip"
  },
  {
    "task": "gemm_naive",
    "instruction": "Optimize the naive GEMM kernel. Current version uses small 8x8 thread blocks, no shared memory, no tiling. Key optimizations: use shared memory for A and B tiles, increase block size, use thread-level tiling (each thread computes multiple elements), loop unrolling. MUST keep kernel name: gemm_naive_kernel and signature: (float* C, const float* A, const float* B, int M, int N, int K).",
    "file_path": "gemm_naive",
    "file_name": "gemm_naive.hip"
  },
  {
    "task": "softmax_naive",
    "instruction": "Optimize the naive softmax kernel. Current version uses only 32 threads per block with sequential reduction in thread 0. Key optimizations: increase block size to 256, use warp shuffle for parallel reduction, use vectorized loads. MUST keep kernel name: softmax_naive_kernel and signature: (float* output, const float* input, int batch_size, int feature_dim).",
    "file_path": "softmax_naive",
    "file_name": "softmax_naive.hip"
  },
  {
    "task": "conv_naive",
    "instruction": "Optimize the naive 2D convolution kernel. Current version has no shared memory, reads kernel weights from global memory repeatedly. Key optimizations: use shared memory for input tiles and kernel weights, use register blocking. MUST keep kernel name: conv2d_naive_kernel.",
    "file_path": "conv_naive",
    "file_name": "conv_naive.hip"
  },
  {
    "task": "attention_naive",
    "instruction": "Optimize the naive attention kernel. Current version uses flat indexing with small block size, no shared memory. Key optimizations: use 2D thread blocks, shared memory tiling for Q/K/V, fused softmax computation. MUST keep kernel names: attention_naive_kernel and attention_softmax_kernel.",
    "file_path": "attention_naive",
    "file_name": "attention_naive.hip"
  },
  {
    "task": "transpose_naive",
    "instruction": "Optimize the naive transpose kernel. Current version uses flat 1D indexing with small block size, causing non-coalesced memory writes. Key optimizations: use 2D thread blocks (e.g., 32x32), use shared memory tile to enable coalesced reads and writes, handle bank conflicts. MUST keep kernel name: transpose_naive_kernel and signature: (float* output, const float* input, int rows, int cols).",
    "file_path": "transpose_naive",
    "file_name": "transpose_naive.hip"
  }
]